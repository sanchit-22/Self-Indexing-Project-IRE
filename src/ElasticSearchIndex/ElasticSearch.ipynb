{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets==4.1.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.1.1)\n",
      "Requirement already satisfied: nltk==3.9.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.9.1)\n",
      "Requirement already satisfied: matplotlib==3.10.6 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (3.10.6)\n",
      "Requirement already satisfied: elasticsearch==8.11.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (8.11.0)\n",
      "Requirement already satisfied: tqdm==4.67.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: scipy==1.16.2 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 6)) (1.16.2)\n",
      "Requirement already satisfied: seaborn==0.13.2 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (0.13.2)\n",
      "Requirement already satisfied: filelock in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from datasets==4.1.1->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: click in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (8.3.0)\n",
      "Requirement already satisfied: joblib in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from nltk==3.9.1->-r requirements.txt (line 2)) (2025.10.23)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from matplotlib==3.10.6->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from elasticsearch==8.11.0->-r requirements.txt (line 4)) (8.17.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.2 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from elastic-transport<9,>=8->elasticsearch==8.11.0->-r requirements.txt (line 4)) (2025.10.5)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==4.1.1->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets==4.1.1->-r requirements.txt (line 1)) (1.1.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from pandas->datasets==4.1.1->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from pandas->datasets==4.1.1->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.10.6->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets==4.1.1->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets==4.1.1->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets==4.1.1->-r requirements.txt (line 1)) (1.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import threading\n",
    "import statistics\n",
    "import traceback\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Data Processing and Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress Bars and UI\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Machine Learning Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# HTTP Requests and Elasticsearch\n",
    "import requests\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# System Monitoring and Performance\n",
    "import psutil\n",
    "import resource\n",
    "\n",
    "# Concurrent Processing\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Random and Math\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Current Configuration:\n",
      "   Dataset: wikipedia\n",
      "   Max Documents: 50000\n",
      "   Index Name: esindex-v1.0\n",
      "   Batch Size: 100\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configuration\n",
    "DATASET_CONFIG = {\n",
    "    'wikipedia': {\n",
    "        'name': 'wikimedia/wikipedia',\n",
    "        'version': '20231101.en',\n",
    "        'text_field': 'text',\n",
    "        'id_field': 'id',\n",
    "        'split': 'train'\n",
    "    },\n",
    "    'news': {\n",
    "        'name': 'custom_news',  # Placeholder for news dataset\n",
    "        'version': None,\n",
    "        'text_field': 'text',\n",
    "        'id_field': 'id',\n",
    "        'split': 'train'\n",
    "    }\n",
    "}\n",
    "\n",
    "# User Configuration - MODIFY THESE VALUES\n",
    "SELECTED_DATASET = 'wikipedia'  # Options: 'wikipedia', 'news'\n",
    "MAX_DOCUMENTS = 50000           # Number of documents to process\n",
    "INDEX_NAME = \"esindex-v1.0\"    # Elasticsearch index name\n",
    "BATCH_SIZE = 100              # Batch size for ES indexing\n",
    "\n",
    "# Display current configuration\n",
    "print(\"üîß Current Configuration:\")\n",
    "print(f\"   Dataset: {SELECTED_DATASET}\")\n",
    "print(f\"   Max Documents: {MAX_DOCUMENTS}\")\n",
    "print(f\"   Index Name: {INDEX_NAME}\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e677b03cab84dd0961a64f6e0ecc7d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53833f153c5c41cea3eed4bd5ed37a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully downloaded/loaded to: /home/san22chit/Documents/IIITH/Sem3/IRE/Assignments/IndexingAndRetrieval/local_wikipedia_data\n"
     ]
    }
   ],
   "source": [
    "# Define the local directory where you want to save the data\n",
    "local_path = os.path.join(os.getcwd(), \"local_wikipedia_data\")\n",
    "\n",
    "# 1. Download and save the data to the specified local_path\n",
    "ds = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.en\",\n",
    "    cache_dir=local_path  \n",
    ")\n",
    "\n",
    "print(f\"Dataset successfully downloaded/loaded to: {local_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing (Hugging Face Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Loading dataset: wikipedia\n",
      "‚úÖ Dataset loaded: 6407814 total documents\n"
     ]
    }
   ],
   "source": [
    "def load_selected_dataset(dataset_key, max_docs=None):\n",
    "    \"\"\"Load the specified dataset with configuration\"\"\"\n",
    "    \n",
    "    if dataset_key not in DATASET_CONFIG:\n",
    "        raise ValueError(f\"Dataset '{dataset_key}' not found. Available: {list(DATASET_CONFIG.keys())}\")\n",
    "    \n",
    "    config = DATASET_CONFIG[dataset_key]\n",
    "    \n",
    "    print(f\"üìÅ Loading dataset: {dataset_key}\")\n",
    "    \n",
    "    if dataset_key == 'wikipedia':\n",
    "        # Use existing Wikipedia dataset\n",
    "        dataset = ds  # Your existing loaded dataset\n",
    "        split_data = dataset[config['split']]\n",
    "        \n",
    "    elif dataset_key == 'news':\n",
    "        # Load news dataset (implement based on your news source)\n",
    "        print(\"‚ö†Ô∏è  Loading sample news data (replace with actual news dataset)\")\n",
    "        # For demo, create sample news data\n",
    "        sample_news = [\n",
    "            {\n",
    "                'id': f'news_{i}',\n",
    "                'text': f\"Sample news article {i}: This is a technology news article about artificial intelligence and machine learning developments in the industry. The latest research shows significant progress in natural language processing.\",\n",
    "                'title': f\"News Article {i}\"\n",
    "            }\n",
    "            for i in range(min(max_docs or 1000, 1000))\n",
    "        ]\n",
    "        \n",
    "        # Convert to dataset-like structure\n",
    "        class SimpleDataset:\n",
    "            def __init__(self, data):\n",
    "                self.data = data\n",
    "            \n",
    "            def __iter__(self):\n",
    "                return iter(self.data)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                return self.data[idx]\n",
    "            \n",
    "            def __len__(self):\n",
    "                return len(self.data)\n",
    "        \n",
    "        split_data = SimpleDataset(sample_news)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded: {len(split_data) if hasattr(split_data, '__len__') else 'Unknown size'} total documents\")\n",
    "    \n",
    "    return split_data, config\n",
    "\n",
    "# Load the selected dataset\n",
    "selected_data, dataset_config = load_selected_dataset(SELECTED_DATASET, MAX_DOCUMENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/san22chit/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/san22chit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources if not already present\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Get English stopwords and initialize stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "punct_table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(punct_table)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and stem\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_counts(split, preprocess_func=None, max_docs=10000):\n",
    "    word_count = Counter()\n",
    "    for idx, item in enumerate(ds[split]):\n",
    "        text = item['text']\n",
    "        if preprocess_func:\n",
    "            tokens = preprocess_func(text)\n",
    "        else:\n",
    "            tokens = word_tokenize(text)\n",
    "        word_count.update(tokens)\n",
    "        if idx+1 >= max_docs:\n",
    "            break\n",
    "    return word_count\n",
    "\n",
    "def plot_word_freq(counter, title, filename, top_n=30):\n",
    "    most_common = counter.most_common(top_n)\n",
    "    words, counts = zip(*most_common)\n",
    "    plt.figure(figsize=(14,7))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Preprocessing 50000 documents from wikipedia dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50000/50000 [06:43<00:00, 123.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 50000 documents\n",
      "   Original vocabulary: 985682 unique tokens\n",
      "   Processed vocabulary: 607460 unique tokens\n",
      "üìä Plots saved for wikipedia dataset with 50000 documents\n",
      "üíæ Saved 50000 processed documents to: dataset/preprocessed_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "def preprocess_selected_dataset(dataset, config, max_docs, preprocess_func=None):\n",
    "    \"\"\"Preprocess the selected dataset with limits\"\"\"\n",
    "    \n",
    "    print(f\"üîÑ Preprocessing {max_docs} documents from {SELECTED_DATASET} dataset...\")\n",
    "    \n",
    "    original_counter = Counter()\n",
    "    processed_counter = Counter()\n",
    "    processed_documents = []\n",
    "    \n",
    "    count = 0\n",
    "    for item in tqdm(dataset, desc=\"Processing documents\", total=max_docs):\n",
    "        if count >= max_docs:\n",
    "            break\n",
    "            \n",
    "        # Extract text based on config\n",
    "        text = item[config['text_field']]\n",
    "        doc_id = item[config['id_field']]\n",
    "        \n",
    "        # Original tokenization\n",
    "        original_tokens = word_tokenize(text)\n",
    "        original_counter.update(original_tokens)\n",
    "        \n",
    "        # Processed tokens\n",
    "        if preprocess_func:\n",
    "            processed_tokens = preprocess_func(text)\n",
    "            processed_counter.update(processed_tokens)\n",
    "        else:\n",
    "            processed_tokens = original_tokens\n",
    "        \n",
    "        # Store processed document\n",
    "        processed_documents.append({\n",
    "            'id': doc_id,\n",
    "            'original_text': text,\n",
    "            'processed_tokens': processed_tokens,\n",
    "            'title': item.get('title', '')\n",
    "        })\n",
    "        \n",
    "        count += 1\n",
    "    \n",
    "    print(f\"‚úÖ Processed {count} documents\")\n",
    "    print(f\"   Original vocabulary: {len(original_counter)} unique tokens\")\n",
    "    print(f\"   Processed vocabulary: {len(processed_counter)} unique tokens\")\n",
    "    \n",
    "    return processed_documents, original_counter, processed_counter\n",
    "\n",
    "# Run preprocessing on selected dataset\n",
    "processed_docs, orig_counts, proc_counts = preprocess_selected_dataset(\n",
    "    selected_data, \n",
    "    dataset_config, \n",
    "    MAX_DOCUMENTS, \n",
    "    preprocess\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Generate plots for selected dataset\n",
    "plot_word_freq(\n",
    "    orig_counts, \n",
    "    f\"Word Frequency Before Preprocessing ({SELECTED_DATASET} - {MAX_DOCUMENTS} docs)\",\n",
    "    f\"freq_before_{SELECTED_DATASET}_{MAX_DOCUMENTS}.png\"\n",
    ")\n",
    "\n",
    "plot_word_freq(\n",
    "    proc_counts, \n",
    "    f\"Word Frequency After Preprocessing ({SELECTED_DATASET} - {MAX_DOCUMENTS} docs)\",\n",
    "    f\"freq_after_{SELECTED_DATASET}_{MAX_DOCUMENTS}.png\"\n",
    ")\n",
    "\n",
    "print(f\"üìä Plots saved for {SELECTED_DATASET} dataset with {MAX_DOCUMENTS} documents\")\n",
    "# Save processed documents to CSV\n",
    "out_dir = Path(\"dataset\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"preprocessed_dataset.csv\"\n",
    "\n",
    "try:\n",
    "    df_pre = pd.DataFrame(processed_docs)\n",
    "    # Normalize processed_tokens to a space-joined string for CSV storage\n",
    "    df_pre['processed_tokens'] = df_pre['processed_tokens'].apply(lambda t: \" \".join(t) if isinstance(t, (list, tuple)) else str(t))\n",
    "    # Add token count for convenience\n",
    "    df_pre['token_count'] = df_pre['processed_tokens'].apply(lambda s: len(s.split()) if s else 0)\n",
    "    df_pre.to_csv(out_path, index=False, encoding='utf-8')\n",
    "    print(f\"üíæ Saved {len(df_pre)} processed documents to: {out_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save processed dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Connecting to Elasticsearch...\n",
      "‚úÖ Elasticsearch connection successful!\n",
      "   Version: 8.11.0\n",
      "   Cluster: docker-cluster\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "\n",
    "class WorkingElasticsearch:\n",
    "    \"\"\"Working Elasticsearch client using requests library\"\"\"\n",
    "    \n",
    "    def __init__(self, host=\"http://localhost:9200\"):\n",
    "        self.host = host.rstrip('/')\n",
    "        \n",
    "    def ping(self):\n",
    "        try:\n",
    "            response = requests.get(f\"{self.host}/\", timeout=10)\n",
    "            return response.status_code == 200\n",
    "        except Exception as e:\n",
    "            print(f\"Ping error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def info(self):\n",
    "        try:\n",
    "            response = requests.get(f\"{self.host}/\", timeout=10)\n",
    "            return response.json() if response.status_code == 200 else None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def delete_index(self, index_name):\n",
    "        try:\n",
    "            response = requests.delete(f\"{self.host}/{quote(index_name)}\")\n",
    "            return response.status_code in [200, 404]\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def create_index(self, index_name, body):\n",
    "        try:\n",
    "            response = requests.put(f\"{self.host}/{quote(index_name)}\", json=body, timeout=30)\n",
    "            return response.status_code in [200, 201]\n",
    "        except Exception as e:\n",
    "            print(f\"Create index error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def index_exists(self, index_name):\n",
    "        try:\n",
    "            response = requests.head(f\"{self.host}/{quote(index_name)}\")\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def bulk_index(self, docs):\n",
    "        \"\"\"Bulk index documents\"\"\"\n",
    "        try:\n",
    "            bulk_data = []\n",
    "            for doc in docs:\n",
    "                # Add index action\n",
    "                bulk_data.append(json.dumps({\n",
    "                    \"index\": {\n",
    "                        \"_index\": doc[\"_index\"],\n",
    "                        \"_id\": doc[\"_id\"]\n",
    "                    }\n",
    "                }))\n",
    "                # Add document data\n",
    "                doc_data = {k: v for k, v in doc.items() if not k.startswith('_')}\n",
    "                bulk_data.append(json.dumps(doc_data))\n",
    "            \n",
    "            bulk_body = '\\n'.join(bulk_data) + '\\n'\n",
    "            headers = {'Content-Type': 'application/x-ndjson'}\n",
    "            \n",
    "            response = requests.post(\n",
    "                f\"{self.host}/_bulk\",\n",
    "                data=bulk_body,\n",
    "                headers=headers,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                errors = [item for item in result.get('items', []) if 'error' in item.get('index', {})]\n",
    "                if errors:\n",
    "                    print(f\"‚ö†Ô∏è  {len(errors)} indexing errors occurred\")\n",
    "                return len(errors) == 0\n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Bulk index error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def refresh_index(self, index_name):\n",
    "        try:\n",
    "            response = requests.post(f\"{self.host}/{quote(index_name)}/_refresh\")\n",
    "            return response.status_code == 200\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def search(self, index_name, body):\n",
    "        try:\n",
    "            response = requests.post(f\"{self.host}/{quote(index_name)}/_search\", json=body, timeout=30)\n",
    "            return response.json() if response.status_code == 200 else None\n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_index_stats(self, index_name):\n",
    "        try:\n",
    "            response = requests.get(f\"{self.host}/{quote(index_name)}/_stats\")\n",
    "            return response.json() if response.status_code == 200 else None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# Create Elasticsearch client\n",
    "print(\"üîå Connecting to Elasticsearch...\")\n",
    "es = WorkingElasticsearch()\n",
    "\n",
    "if es.ping():\n",
    "    print(\"‚úÖ Elasticsearch connection successful!\")\n",
    "    info = es.info()\n",
    "    if info:\n",
    "        print(f\"   Version: {info['version']['number']}\")\n",
    "        print(f\"   Cluster: {info['cluster_name']}\")\n",
    "else:\n",
    "    print(\"‚ùå Cannot connect to Elasticsearch\")\n",
    "    print(\"   Make sure Docker container is running: docker ps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üöÄ RUNNING OPTIMIZED BULK INDEXING\n",
      "============================================================\n",
      "üîß Optimized indexing for large documents...\n",
      "   Batch size: 5\n",
      "   Total documents: 50000\n",
      "üóëÔ∏è  Deleted existing index\n",
      "‚úÖ Created optimized index: esindex-v1.0\n",
      "üìã Prepared 50000 documents (skipped 0)\n",
      "üîÑ Indexing in 10000 batches of 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bulk indexing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [01:39<00:00, 100.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexing complete: 50000/50000 documents\n",
      "üìä Index stats: 50000 documents, 124.48 MB\n",
      "\n",
      "üìä Indexing Results:\n",
      "   Success: ‚úÖ\n",
      "   Duration: 101.31 seconds\n",
      "   Index: esindex-v1.0\n",
      "\n",
      "üéâ ESIndex-v1.0 Successfully Created!\n"
     ]
    }
   ],
   "source": [
    "def optimized_bulk_indexing(processed_documents, index_name, es_client, batch_size=10):\n",
    "    \"\"\"\n",
    "    Optimized bulk indexing that handles large documents properly\n",
    "    \"\"\"\n",
    "    \n",
    "    if not es_client.ping():\n",
    "        print(\"‚ùå No Elasticsearch connection\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"üîß Optimized indexing for large documents...\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    print(f\"   Total documents: {len(processed_documents)}\")\n",
    "    \n",
    "    # Delete and recreate index with optimized settings\n",
    "    if es_client.index_exists(index_name):\n",
    "        es_client.delete_index(index_name)\n",
    "        print(f\"üóëÔ∏è  Deleted existing index\")\n",
    "    \n",
    "    # Optimized mapping for large documents\n",
    "    mapping = {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"id\": {\"type\": \"keyword\"},\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"standard\",\n",
    "                    \"index_options\": \"docs\",  # Don't store positions/frequencies\n",
    "                    \"norms\": False  # Disable scoring norms to save space\n",
    "                },\n",
    "                \"title\": {\"type\": \"text\"},\n",
    "                \"token_count\": {\"type\": \"integer\"}\n",
    "            }\n",
    "        },\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 0,\n",
    "            \"refresh_interval\": \"30s\",  # Reduce refresh frequency\n",
    "            \"index\": {\n",
    "                \"max_result_window\": 10000,\n",
    "                \"mapping\": {\n",
    "                    \"total_fields\": {\"limit\": 1000}\n",
    "                },\n",
    "                \"blocks\": {\n",
    "                    \"read_only_allow_delete\": False\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not es_client.create_index(index_name, mapping):\n",
    "        print(f\"‚ùå Failed to create optimized index\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"‚úÖ Created optimized index: {index_name}\")\n",
    "    \n",
    "    # Prepare documents with size optimization\n",
    "    optimized_docs = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for i, doc in enumerate(processed_documents):\n",
    "        try:\n",
    "            text = doc['original_text']\n",
    "            \n",
    "            # Aggressive text size limiting for bulk operations\n",
    "            max_size = 10000  # 10KB per document for bulk operations\n",
    "            if len(text) > max_size:\n",
    "                text = text[:max_size] + \"... [truncated]\"\n",
    "            \n",
    "            # Clean document ID\n",
    "            doc_id = str(doc['id']).replace('/', '_').replace(' ', '_')[:50]\n",
    "            \n",
    "            optimized_doc = {\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": doc_id,\n",
    "                \"id\": doc_id,\n",
    "                \"text\": text,\n",
    "                \"title\": str(doc.get('title', ''))[:200],\n",
    "                \"token_count\": len(doc['processed_tokens'])\n",
    "            }\n",
    "            \n",
    "            optimized_docs.append(optimized_doc)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Skipping doc {i}: {e}\")\n",
    "            skipped += 1\n",
    "    \n",
    "    print(f\"üìã Prepared {len(optimized_docs)} documents (skipped {skipped})\")\n",
    "    \n",
    "    # Bulk index with very small batches and error handling\n",
    "    total_success = 0\n",
    "    total_batches = (len(optimized_docs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"üîÑ Indexing in {total_batches} batches of {batch_size}...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(optimized_docs), batch_size), desc=\"Bulk indexing\"):\n",
    "        batch = optimized_docs[i:i + batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        try:\n",
    "            # Create bulk request\n",
    "            bulk_lines = []\n",
    "            for doc in batch:\n",
    "                # Index action\n",
    "                bulk_lines.append(json.dumps({\n",
    "                    \"index\": {\n",
    "                        \"_index\": doc[\"_index\"],\n",
    "                        \"_id\": doc[\"_id\"]\n",
    "                    }\n",
    "                }))\n",
    "                # Document\n",
    "                doc_data = {k: v for k, v in doc.items() if not k.startswith('_')}\n",
    "                bulk_lines.append(json.dumps(doc_data))\n",
    "            \n",
    "            bulk_body = '\\n'.join(bulk_lines) + '\\n'\n",
    "            \n",
    "            # Send bulk request with retries\n",
    "            max_retries = 3\n",
    "            for retry in range(max_retries):\n",
    "                try:\n",
    "                    response = requests.post(\n",
    "                        f\"{es_client.host}/_bulk\",\n",
    "                        data=bulk_body,\n",
    "                        headers={'Content-Type': 'application/x-ndjson'},\n",
    "                        timeout=120  # Longer timeout\n",
    "                    )\n",
    "                    \n",
    "                    if response.status_code == 200:\n",
    "                        result = response.json()\n",
    "                        \n",
    "                        # Count successful indexings\n",
    "                        batch_success = 0\n",
    "                        batch_errors = 0\n",
    "                        \n",
    "                        for item in result.get('items', []):\n",
    "                            if 'error' in item.get('index', {}):\n",
    "                                batch_errors += 1\n",
    "                                # Only show first error per batch\n",
    "                                if batch_errors == 1:\n",
    "                                    error = item['index']['error']\n",
    "                                    print(f\"‚ùå Batch {batch_num} error: {error.get('type', 'unknown')}\")\n",
    "                            else:\n",
    "                                batch_success += 1\n",
    "                        \n",
    "                        total_success += batch_success\n",
    "                        \n",
    "                        if batch_errors == 0:\n",
    "                            break  # Success, no need to retry\n",
    "                        elif batch_success > 0:\n",
    "                            print(f\"‚ö†Ô∏è  Batch {batch_num}: {batch_success} success, {batch_errors} errors\")\n",
    "                            break  # Partial success, move on\n",
    "                        else:\n",
    "                            print(f\"‚ùå Batch {batch_num}: All failed, retry {retry + 1}/{max_retries}\")\n",
    "                            if retry == max_retries - 1:\n",
    "                                print(f\"üíÄ Batch {batch_num}: Giving up after {max_retries} retries\")\n",
    "                    else:\n",
    "                        print(f\"‚ùå Batch {batch_num}: HTTP {response.status_code}\")\n",
    "                        if retry == max_retries - 1:\n",
    "                            print(f\"Response: {response.text[:200]}\")\n",
    "                    \n",
    "                    if retry < max_retries - 1:\n",
    "                        time.sleep(2)  # Wait before retry\n",
    "                        \n",
    "                except requests.exceptions.Timeout:\n",
    "                    print(f\"‚è∞ Batch {batch_num}: Timeout (retry {retry + 1}/{max_retries})\")\n",
    "                    if retry < max_retries - 1:\n",
    "                        time.sleep(5)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Batch {batch_num}: Exception {e}\")\n",
    "                    break\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Critical error in batch {batch_num}: {e}\")\n",
    "    \n",
    "    print(f\"‚úÖ Indexing complete: {total_success}/{len(optimized_docs)} documents\")\n",
    "    \n",
    "    # Refresh and get stats\n",
    "    es_client.refresh_index(index_name)\n",
    "    \n",
    "    stats = es_client.get_index_stats(index_name)\n",
    "    if stats and 'indices' in stats:\n",
    "        actual_count = stats['indices'][index_name]['total']['docs']['count']\n",
    "        size_mb = stats['indices'][index_name]['total']['store']['size_in_bytes'] / (1024 * 1024)\n",
    "        print(f\"üìä Index stats: {actual_count} documents, {size_mb:.2f} MB\")\n",
    "        \n",
    "        # Success if we have at least 70% of documents\n",
    "        success_rate = actual_count / len(processed_docs) if processed_docs else 0\n",
    "        return success_rate >= 0.7\n",
    "    \n",
    "    return total_success >= len(optimized_docs) * 0.7\n",
    "\n",
    "# Run optimized indexing\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"üöÄ RUNNING OPTIMIZED BULK INDEXING\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "import time  # Add this import\n",
    "\n",
    "if es and es.ping():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimized_success = optimized_bulk_indexing(\n",
    "        processed_docs, \n",
    "        INDEX_NAME, \n",
    "        es, \n",
    "        batch_size=5  # Very small batches for large documents\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\nüìä Indexing Results:\")\n",
    "    print(f\"   Success: {'‚úÖ' if optimized_success else '‚ùå'}\")\n",
    "    print(f\"   Duration: {duration:.2f} seconds\")\n",
    "    print(f\"   Index: {INDEX_NAME}\")\n",
    "    \n",
    "    if optimized_success:\n",
    "        print(f\"\\nüéâ ESIndex-v1.0 Successfully Created!\")\n",
    "        indexing_success = True\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Indexing completed with reduced success rate\")\n",
    "        indexing_success = optimized_success\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No Elasticsearch connection\")\n",
    "    indexing_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing optimized index: esindex-v1.0\n",
      "========================================\n",
      "üìä Total documents in index: 50000\n",
      "\n",
      "üîç Simple match:\n",
      "   Results: 25 total\n",
      "   1. Anarchism (score: 12.796, tokens: 3970)\n",
      "      Anarchism is a political philosophy and movement that is skeptical of all justif...\n",
      "   2. Ayn Rand (score: 12.796, tokens: 3615)\n",
      "      Alice O'Connor (born Alisa Zinovyevna Rosenbaum; , 1905¬†‚Äì March 6, 1982), better...\n",
      "\n",
      "üîç Title search:\n",
      "   Results: 2 total\n",
      "   1. Anarchism (score: 13.429, tokens: 3970)\n",
      "      Anarchism is a political philosophy and movement that is skeptical of all justif...\n",
      "   2. Anarchism in Mexico (score: 9.610, tokens: 917)\n",
      "      Anarchism in Mexico, the anarchist movement in Mexico, extends from Plotino Rhod...\n",
      "\n",
      "üîç Multi-word:\n",
      "   Results: 4801 total\n",
      "   1. Anarchism (score: 10.843, tokens: 3970)\n",
      "      Anarchism is a political philosophy and movement that is skeptical of all justif...\n",
      "   2. Ayn Rand (score: 10.843, tokens: 3615)\n",
      "      Alice O'Connor (born Alisa Zinovyevna Rosenbaum; , 1905¬†‚Äì March 6, 1982), better...\n",
      "\n",
      "üîç Range query:\n",
      "   Results: 10000 total\n",
      "   1. Anarchism (score: 1.000, tokens: 3970)\n",
      "      Anarchism is a political philosophy and movement that is skeptical of all justif...\n",
      "   2. Albedo (score: 1.000, tokens: 2300)\n",
      "      Albedo (; ) is the fraction of sunlight that is diffusely reflected by a body. I...\n"
     ]
    }
   ],
   "source": [
    "def test_optimized_index(es_client, index_name):\n",
    "    \"\"\"Test the optimized index with various queries\"\"\"\n",
    "    \n",
    "    if not es_client.ping():\n",
    "        print(\"‚ùå No ES connection for testing\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüß™ Testing optimized index: {index_name}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Basic stats\n",
    "    try:\n",
    "        stats = es_client.get_index_stats(index_name)\n",
    "        if stats and 'indices' in stats:\n",
    "            doc_count = stats['indices'][index_name]['total']['docs']['count']\n",
    "            print(f\"üìä Total documents in index: {doc_count}\")\n",
    "        else:\n",
    "            print(\"‚ùå Could not get index stats\")\n",
    "            return\n",
    "    except:\n",
    "        print(\"‚ùå Index may not exist\")\n",
    "        return\n",
    "    \n",
    "    # Test queries\n",
    "    test_queries = [\n",
    "        (\"Simple match\", {\"query\": {\"match\": {\"text\": \"anarchism\"}}, \"size\": 3}),\n",
    "        (\"Title search\", {\"query\": {\"match\": {\"title\": \"anarchism\"}}, \"size\": 3}),\n",
    "        (\"Multi-word\", {\"query\": {\"match\": {\"text\": \"political philosophy\"}}, \"size\": 3}),\n",
    "        (\"Range query\", {\"query\": {\"range\": {\"token_count\": {\"gte\": 100}}}, \"size\": 5})\n",
    "    ]\n",
    "    \n",
    "    for query_name, query in test_queries:\n",
    "        try:\n",
    "            print(f\"\\nüîç {query_name}:\")\n",
    "            results = es_client.search(index_name, query)\n",
    "            \n",
    "            if results and 'hits' in results:\n",
    "                total = results['hits']['total']['value']\n",
    "                hits = results['hits']['hits']\n",
    "                \n",
    "                print(f\"   Results: {total} total\")\n",
    "                for i, hit in enumerate(hits[:2], 1):\n",
    "                    score = hit.get('_score', 0)\n",
    "                    title = hit['_source'].get('title', 'No title')\n",
    "                    tokens = hit['_source'].get('token_count', 0)\n",
    "                    text_preview = hit['_source']['text'][:80] + \"...\"\n",
    "                    \n",
    "                    print(f\"   {i}. {title} (score: {score:.3f}, tokens: {tokens})\")\n",
    "                    print(f\"      {text_preview}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå No results or error\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Query error: {e}\")\n",
    "\n",
    "# Test the optimized index\n",
    "if indexing_success:\n",
    "    test_optimized_index(es, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Performance Metrics System Initialized\n",
      "   System: ESIndex-v1.0\n",
      "   Start Time: 2025-10-28 17:44:09\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import psutil\n",
    "import statistics\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class PerformanceMetrics:\n",
    "    \"\"\"Comprehensive performance metrics collector for search systems\"\"\"\n",
    "    \n",
    "    def __init__(self, system_name=\"ESIndex-v1.0\"):\n",
    "        self.system_name = system_name\n",
    "        self.query_times = []\n",
    "        self.memory_usage = []\n",
    "        self.throughput_data = []\n",
    "        self.functional_metrics = {}\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset all collected metrics\"\"\"\n",
    "        self.query_times = []\n",
    "        self.memory_usage = []\n",
    "        self.throughput_data = []\n",
    "        self.functional_metrics = {}\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def record_query_time(self, query_time_ms):\n",
    "        \"\"\"Record individual query response time\"\"\"\n",
    "        self.query_times.append(query_time_ms)\n",
    "        \n",
    "    def record_memory_usage(self):\n",
    "        \"\"\"Record current system memory usage\"\"\"\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        self.memory_usage.append({\n",
    "            'timestamp': time.time() - self.start_time,\n",
    "            'memory_mb': memory_mb,\n",
    "            'memory_percent': process.memory_percent()\n",
    "        })\n",
    "        return memory_mb\n",
    "        \n",
    "    def calculate_latency_percentiles(self):\n",
    "        \"\"\"Calculate A: Latency percentiles (p95, p99)\"\"\"\n",
    "        if not self.query_times:\n",
    "            return None\n",
    "            \n",
    "        sorted_times = sorted(self.query_times)\n",
    "        n = len(sorted_times)\n",
    "        \n",
    "        percentiles = {\n",
    "            'p50': np.percentile(sorted_times, 50),\n",
    "            'p90': np.percentile(sorted_times, 90), \n",
    "            'p95': np.percentile(sorted_times, 95),\n",
    "            'p99': np.percentile(sorted_times, 99),\n",
    "            'mean': statistics.mean(sorted_times),\n",
    "            'min': min(sorted_times),\n",
    "            'max': max(sorted_times),\n",
    "            'total_queries': n\n",
    "        }\n",
    "        \n",
    "        return percentiles\n",
    "        \n",
    "    def calculate_throughput(self, duration_seconds):\n",
    "        \"\"\"Calculate B: Throughput (queries/second)\"\"\"\n",
    "        if not self.query_times or duration_seconds <= 0:\n",
    "            return 0\n",
    "            \n",
    "        return len(self.query_times) / duration_seconds\n",
    "        \n",
    "    def get_memory_footprint(self):\n",
    "        \"\"\"Calculate C: Memory footprint statistics\"\"\"\n",
    "        if not self.memory_usage:\n",
    "            return None\n",
    "            \n",
    "        memory_values = [m['memory_mb'] for m in self.memory_usage]\n",
    "        \n",
    "        return {\n",
    "            'peak_memory_mb': max(memory_values),\n",
    "            'average_memory_mb': statistics.mean(memory_values),\n",
    "            'min_memory_mb': min(memory_values),\n",
    "            'memory_growth_mb': memory_values[-1] - memory_values[0] if len(memory_values) > 1 else 0\n",
    "        }\n",
    "\n",
    "# Initialize performance metrics\n",
    "perf_metrics = PerformanceMetrics(\"ESIndex-v1.0\")\n",
    "print(\"üöÄ Performance Metrics System Initialized\")\n",
    "print(f\"   System: {perf_metrics.system_name}\")\n",
    "print(f\"   Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Generating Diverse Query Set...\n",
      "   Rationale: Testing different search patterns and system behaviors\n",
      "‚úÖ Generated 43 diverse queries across 10 categories\n",
      "\n",
      "üìã QUERY SET JUSTIFICATION:\n",
      "   single_term: Test basic term matching and TF-IDF scoring\n",
      "   multi_term: Test multi-term coordination and ranking\n",
      "   phrase_queries: Test exact phrase matching and position indexing\n",
      "   long_queries: Test system performance with complex queries\n",
      "   rare_terms: Test handling of low-frequency terms\n",
      "   common_terms: Test system behavior with high-frequency terms\n",
      "   boolean_queries: Test Boolean query processing\n",
      "   range_queries: Test numeric range operations\n",
      "   wildcard_fuzzy: Test pattern matching and fuzzy search\n",
      "   empty_no_results: Test system behavior with no matches\n",
      "\n",
      "üéØ Query Distribution:\n",
      "   single_term: 5 queries\n",
      "   multi_term: 5 queries\n",
      "   phrase_queries: 5 queries\n",
      "   long_queries: 4 queries\n",
      "   rare_terms: 5 queries\n",
      "   common_terms: 5 queries\n",
      "   boolean_queries: 4 queries\n",
      "   range_queries: 3 queries\n",
      "   wildcard_fuzzy: 4 queries\n",
      "   empty_no_results: 3 queries\n"
     ]
    }
   ],
   "source": [
    "def generate_diverse_query_set():\n",
    "    \"\"\"\n",
    "    Generate a diverse query set that tests various system properties\n",
    "    Based on information retrieval best practices and system stress testing\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üß† Generating Diverse Query Set...\")\n",
    "    print(\"   Rationale: Testing different search patterns and system behaviors\")\n",
    "    \n",
    "    # Query categories with justification\n",
    "    query_categories = {\n",
    "        'single_term': {\n",
    "            'queries': ['anarchism', 'philosophy', 'politics', 'government', 'society'],\n",
    "            'purpose': 'Test basic term matching and TF-IDF scoring',\n",
    "            'system_property': 'Core search functionality, index lookup efficiency'\n",
    "        },\n",
    "        \n",
    "        'multi_term': {\n",
    "            'queries': [\n",
    "                'political philosophy movement',\n",
    "                'artificial intelligence research', \n",
    "                'computer science technology',\n",
    "                'social economic theory',\n",
    "                'historical cultural development'\n",
    "            ],\n",
    "            'purpose': 'Test multi-term coordination and ranking',\n",
    "            'system_property': 'Query processing complexity, Boolean operations'\n",
    "        },\n",
    "        \n",
    "        'phrase_queries': {\n",
    "            'queries': [\n",
    "                '\"political philosophy\"',\n",
    "                '\"artificial intelligence\"', \n",
    "                '\"social movement\"',\n",
    "                '\"economic theory\"',\n",
    "                '\"cultural development\"'\n",
    "            ],\n",
    "            'purpose': 'Test exact phrase matching and position indexing',\n",
    "            'system_property': 'Positional indexing, phrase query optimization'\n",
    "        },\n",
    "        \n",
    "        'long_queries': {\n",
    "            'queries': [\n",
    "                'anarchism political philosophy movement skeptical authority hierarchical power structures',\n",
    "                'artificial intelligence machine learning natural language processing computer science research',\n",
    "                'social economic political cultural historical development theory practice implementation',\n",
    "                'government authority power control society individual freedom liberty rights democracy'\n",
    "            ],\n",
    "            'purpose': 'Test system performance with complex queries',\n",
    "            'system_property': 'Query parser efficiency, memory usage, scoring complexity'\n",
    "        },\n",
    "        \n",
    "        'rare_terms': {\n",
    "            'queries': ['epistemology', 'ontology', 'phenomenology', 'hermeneutics', 'dialectics'],\n",
    "            'purpose': 'Test handling of low-frequency terms',\n",
    "            'system_property': 'Index efficiency for rare terms, IDF calculation'\n",
    "        },\n",
    "        \n",
    "        'common_terms': {\n",
    "            'queries': ['the', 'and', 'of', 'to', 'in'],\n",
    "            'purpose': 'Test system behavior with high-frequency terms',\n",
    "            'system_property': 'Stopword handling, performance with common terms'\n",
    "        },\n",
    "        \n",
    "        'boolean_queries': {\n",
    "            'queries': [\n",
    "                'anarchism AND philosophy',\n",
    "                'politics OR government', \n",
    "                'society NOT authority',\n",
    "                '(political AND philosophy) OR (social AND movement)'\n",
    "            ],\n",
    "            'purpose': 'Test Boolean query processing',\n",
    "            'system_property': 'Boolean logic implementation, query optimization'\n",
    "        },\n",
    "        \n",
    "        'range_queries': {\n",
    "            'queries': [\n",
    "                'token_count:[100 TO 500]',\n",
    "                'token_count:[1000 TO *]',\n",
    "                'token_count:[* TO 100]'\n",
    "            ],\n",
    "            'purpose': 'Test numeric range operations',\n",
    "            'system_property': 'Numeric indexing, range query performance'\n",
    "        },\n",
    "        \n",
    "        'wildcard_fuzzy': {\n",
    "            'queries': [\n",
    "                'politic*',\n",
    "                'philosoph*', \n",
    "                'govern*',\n",
    "                'anarch~2'  # Fuzzy search\n",
    "            ],\n",
    "            'purpose': 'Test pattern matching and fuzzy search',\n",
    "            'system_property': 'Term expansion, fuzzy matching algorithms'\n",
    "        },\n",
    "        \n",
    "        'empty_no_results': {\n",
    "            'queries': [\n",
    "                'xyzabc123nonexistent',\n",
    "                'qqqqwwwweeeerrrr',\n",
    "                'zzzzzaaaabbbbcccc'\n",
    "            ],\n",
    "            'purpose': 'Test system behavior with no matches',\n",
    "            'system_property': 'Graceful handling of empty results, error conditions'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Flatten queries with metadata\n",
    "    all_queries = []\n",
    "    for category, info in query_categories.items():\n",
    "        for query in info['queries']:\n",
    "            all_queries.append({\n",
    "                'query': query,\n",
    "                'category': category,\n",
    "                'purpose': info['purpose'],\n",
    "                'system_property': info['system_property']\n",
    "            })\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(all_queries)} diverse queries across {len(query_categories)} categories\")\n",
    "    \n",
    "    # Print justification\n",
    "    print(\"\\nüìã QUERY SET JUSTIFICATION:\")\n",
    "    for category, info in query_categories.items():\n",
    "        print(f\"   {category}: {info['purpose']}\")\n",
    "    \n",
    "    return all_queries, query_categories\n",
    "\n",
    "# Generate the diverse query set\n",
    "diverse_queries, query_categories = generate_diverse_query_set()\n",
    "\n",
    "print(f\"\\nüéØ Query Distribution:\")\n",
    "for category in query_categories:\n",
    "    count = len([q for q in diverse_queries if q['category'] == category])\n",
    "    print(f\"   {category}: {count} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ MEASURING SYSTEM THROUGHPUT (Metric B)\n",
      "==================================================\n",
      "üìä Throughput test configuration:\n",
      "   Duration: 30 seconds\n",
      "   Query pool size: 20 unique queries\n",
      "   Target: Maximum queries/second\n",
      "\n",
      "üßµ Single-threaded throughput test...\n",
      "‚úÖ Single-threaded results:\n",
      "   Queries executed: 4298\n",
      "   Duration: 30.02 seconds\n",
      "   üéØ Throughput: 143.19 queries/second\n",
      "   Average query time: 6.97 ms\n",
      "\n",
      "üîÄ Multi-threaded throughput test (4 threads)...\n",
      "‚úÖ Multi-threaded results:\n",
      "   Threads: 4\n",
      "   Queries executed: 9132\n",
      "   Duration: 30.03 seconds\n",
      "   üéØ Throughput: 304.13 queries/second\n",
      "   Speedup: 2.12x\n"
     ]
    }
   ],
   "source": [
    "def measure_system_throughput(es_client, index_name, query_set, duration_seconds=30):\n",
    "    \"\"\"\n",
    "    Measure B: System throughput in queries/second\n",
    "    Tests both read operations and mixed workloads\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üöÄ MEASURING SYSTEM THROUGHPUT (Metric B)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Prepare test queries (cycle through diverse set)\n",
    "    test_queries = []\n",
    "    for query_info in query_set[:20]:  # Use first 20 diverse queries\n",
    "        query_text = query_info['query'].replace('\"', '')  # Simplify for throughput test\n",
    "        test_queries.append({\n",
    "            \"query\": {\"match\": {\"text\": query_text}}, \n",
    "            \"size\": 5  # Smaller result set for faster processing\n",
    "        })\n",
    "    \n",
    "    print(f\"üìä Throughput test configuration:\")\n",
    "    print(f\"   Duration: {duration_seconds} seconds\")\n",
    "    print(f\"   Query pool size: {len(test_queries)} unique queries\")\n",
    "    print(f\"   Target: Maximum queries/second\")\n",
    "    \n",
    "    # Single-threaded throughput test\n",
    "    print(f\"\\nüßµ Single-threaded throughput test...\")\n",
    "    single_thread_results = []\n",
    "    start_time = time.time()\n",
    "    query_count = 0\n",
    "    \n",
    "    perf_metrics.record_memory_usage()\n",
    "    \n",
    "    while (time.time() - start_time) < duration_seconds:\n",
    "        # Cycle through queries\n",
    "        query = test_queries[query_count % len(test_queries)]\n",
    "        \n",
    "        try:\n",
    "            query_start = time.time()\n",
    "            result = es_client.search(index_name, query)\n",
    "            query_end = time.time()\n",
    "            \n",
    "            single_thread_results.append({\n",
    "                'query_time': (query_end - query_start) * 1000,\n",
    "                'result_count': result['hits']['total']['value'] if result else 0,\n",
    "                'timestamp': query_end - start_time\n",
    "            })\n",
    "            \n",
    "            query_count += 1\n",
    "            \n",
    "            # Record memory every 50 queries\n",
    "            if query_count % 50 == 0:\n",
    "                perf_metrics.record_memory_usage()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Throughput query failed: {e}\")\n",
    "            \n",
    "    single_thread_duration = time.time() - start_time\n",
    "    single_thread_qps = len(single_thread_results) / single_thread_duration\n",
    "    \n",
    "    print(f\"‚úÖ Single-threaded results:\")\n",
    "    print(f\"   Queries executed: {len(single_thread_results)}\")\n",
    "    print(f\"   Duration: {single_thread_duration:.2f} seconds\")\n",
    "    print(f\"   üéØ Throughput: {single_thread_qps:.2f} queries/second\")\n",
    "    print(f\"   Average query time: {statistics.mean([r['query_time'] for r in single_thread_results]):.2f} ms\")\n",
    "    \n",
    "    # Multi-threaded throughput test\n",
    "    print(f\"\\nüîÄ Multi-threaded throughput test (4 threads)...\")\n",
    "    \n",
    "    def worker_thread(thread_id, duration, results_list):\n",
    "        \"\"\"Worker thread for concurrent throughput testing\"\"\"\n",
    "        thread_start = time.time()\n",
    "        thread_query_count = 0\n",
    "        \n",
    "        while (time.time() - thread_start) < duration:\n",
    "            query = test_queries[thread_query_count % len(test_queries)]\n",
    "            \n",
    "            try:\n",
    "                query_start = time.time()\n",
    "                result = es_client.search(index_name, query)\n",
    "                query_end = time.time()\n",
    "                \n",
    "                results_list.append({\n",
    "                    'thread_id': thread_id,\n",
    "                    'query_time': (query_end - query_start) * 1000,\n",
    "                    'result_count': result['hits']['total']['value'] if result else 0,\n",
    "                    'timestamp': query_end - thread_start\n",
    "                })\n",
    "                \n",
    "                thread_query_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Silently handle errors in stress test\n",
    "                pass\n",
    "    \n",
    "    # Run concurrent threads\n",
    "    multi_thread_results = []\n",
    "    threads = []\n",
    "    thread_count = 4\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(thread_count):\n",
    "        thread = threading.Thread(\n",
    "            target=worker_thread, \n",
    "            args=(i, duration_seconds, multi_thread_results)\n",
    "        )\n",
    "        thread.start()\n",
    "        threads.append(thread)\n",
    "    \n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    multi_thread_duration = time.time() - start_time\n",
    "    multi_thread_qps = len(multi_thread_results) / multi_thread_duration\n",
    "    \n",
    "    print(f\"‚úÖ Multi-threaded results:\")\n",
    "    print(f\"   Threads: {thread_count}\")\n",
    "    print(f\"   Queries executed: {len(multi_thread_results)}\")\n",
    "    print(f\"   Duration: {multi_thread_duration:.2f} seconds\")\n",
    "    print(f\"   üéØ Throughput: {multi_thread_qps:.2f} queries/second\")\n",
    "    print(f\"   Speedup: {multi_thread_qps/single_thread_qps:.2f}x\")\n",
    "    \n",
    "    # Calculate final throughput metrics\n",
    "    throughput_metrics = {\n",
    "        'single_thread_qps': single_thread_qps,\n",
    "        'multi_thread_qps': multi_thread_qps,\n",
    "        'speedup_factor': multi_thread_qps/single_thread_qps if single_thread_qps > 0 else 0,\n",
    "        'thread_count': thread_count,\n",
    "        'test_duration': duration_seconds,\n",
    "        'total_queries': len(single_thread_results) + len(multi_thread_results)\n",
    "    }\n",
    "    \n",
    "    return throughput_metrics, single_thread_results, multi_thread_results\n",
    "\n",
    "# Run throughput measurements\n",
    "if es and es.ping() and indexing_success:\n",
    "    throughput_metrics, single_results, multi_results = measure_system_throughput(\n",
    "        es, INDEX_NAME, diverse_queries, duration_seconds=30\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå Cannot measure throughput - ES not available or indexing failed\")\n",
    "    throughput_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è  MEASURING SYSTEM LATENCY (Metric A)\n",
      "==================================================\n",
      "üî• Warmup phase: 5 runs...\n",
      "üèÉ Starting latency measurements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Measuring latency: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43/43 [00:00<00:00, 66.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä LATENCY RESULTS:\n",
      "   Total queries tested: 43\n",
      "   Mean latency: 14.90 ms\n",
      "   Median (p50): 14.24 ms\n",
      "   90th percentile (p90): 24.52 ms\n",
      "   üéØ 95th percentile (p95): 26.57 ms\n",
      "   üéØ 99th percentile (p99): 27.42 ms\n",
      "   Min latency: 3.70 ms\n",
      "   Max latency: 27.59 ms\n",
      "\n",
      "üìã LATENCY BY QUERY CATEGORY:\n",
      "   single_term          | Avg:  13.83 ms | p95:  14.47 ms | Count: 5\n",
      "   multi_term           | Avg:  18.92 ms | p95:  20.78 ms | Count: 5\n",
      "   phrase_queries       | Avg:  16.61 ms | p95:  19.50 ms | Count: 5\n",
      "   long_queries         | Avg:  26.31 ms | p95:  27.10 ms | Count: 4\n",
      "   rare_terms           | Avg:  12.65 ms | p95:  14.86 ms | Count: 5\n",
      "   common_terms         | Avg:  12.76 ms | p95:  13.38 ms | Count: 5\n",
      "   boolean_queries      | Avg:  19.50 ms | p95:  26.23 ms | Count: 4\n",
      "   range_queries        | Avg:  11.74 ms | p95:  14.42 ms | Count: 3\n",
      "   wildcard_fuzzy       | Avg:   9.03 ms | p95:  14.77 ms | Count: 4\n",
      "   empty_no_results     | Avg:   4.06 ms | p95:   4.46 ms | Count: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def measure_system_latency(es_client, index_name, query_set, warmup_runs=5):\n",
    "    \"\"\"\n",
    "    Measure A: System response time with p95 and p99 percentiles\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"‚è±Ô∏è  MEASURING SYSTEM LATENCY (Metric A)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Warmup phase to eliminate cold start effects\n",
    "    print(f\"üî• Warmup phase: {warmup_runs} runs...\")\n",
    "    warmup_query = {\"query\": {\"match\": {\"text\": \"test\"}}, \"size\": 1}\n",
    "    \n",
    "    for i in range(warmup_runs):\n",
    "        try:\n",
    "            es_client.search(index_name, warmup_query)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"üèÉ Starting latency measurements...\")\n",
    "    \n",
    "    latency_results = []\n",
    "    category_latencies = defaultdict(list)\n",
    "    \n",
    "    # Record memory before testing\n",
    "    perf_metrics.record_memory_usage()\n",
    "    \n",
    "    for i, query_info in enumerate(tqdm(query_set, desc=\"Measuring latency\")):\n",
    "        query_text = query_info['query']\n",
    "        category = query_info['category']\n",
    "        \n",
    "        try:\n",
    "            # Construct appropriate ES query based on type\n",
    "            if category == 'boolean_queries':\n",
    "                # Skip complex boolean for now, use simple match\n",
    "                es_query = {\"query\": {\"match\": {\"text\": query_text.replace(' AND ', ' ').replace(' OR ', ' ').replace(' NOT ', ' ')}}, \"size\": 10}\n",
    "            elif category == 'range_queries':\n",
    "                if 'token_count' in query_text:\n",
    "                    es_query = {\"query\": {\"range\": {\"token_count\": {\"gte\": 100, \"lte\": 1000}}}, \"size\": 10}\n",
    "                else:\n",
    "                    es_query = {\"query\": {\"match\": {\"text\": query_text}}, \"size\": 10}\n",
    "            elif category == 'wildcard_fuzzy':\n",
    "                # Simplify wildcards for basic implementation\n",
    "                clean_query = query_text.replace('*', '').replace('~2', '')\n",
    "                es_query = {\"query\": {\"match\": {\"text\": clean_query}}, \"size\": 10}\n",
    "            else:\n",
    "                # Standard match query\n",
    "                clean_query = query_text.replace('\"', '')  # Remove quotes for basic match\n",
    "                es_query = {\"query\": {\"match\": {\"text\": clean_query}}, \"size\": 10}\n",
    "            \n",
    "            # Measure query time\n",
    "            start_time = time.time()\n",
    "            results = es_client.search(index_name, es_query)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Calculate latency in milliseconds\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            \n",
    "            # Record results\n",
    "            latency_results.append({\n",
    "                'query': query_text,\n",
    "                'category': category,\n",
    "                'latency_ms': latency_ms,\n",
    "                'result_count': results['hits']['total']['value'] if results else 0,\n",
    "                'has_results': results is not None and results['hits']['total']['value'] > 0\n",
    "            })\n",
    "            \n",
    "            category_latencies[category].append(latency_ms)\n",
    "            perf_metrics.record_query_time(latency_ms)\n",
    "            \n",
    "            # Record memory periodically\n",
    "            if i % 10 == 0:\n",
    "                perf_metrics.record_memory_usage()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Query failed: {query_text[:30]}... Error: {e}\")\n",
    "            # Record failed query with high latency\n",
    "            latency_results.append({\n",
    "                'query': query_text,\n",
    "                'category': category,\n",
    "                'latency_ms': 5000,  # 5 second penalty for failed queries\n",
    "                'result_count': 0,\n",
    "                'has_results': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate overall percentiles\n",
    "    percentiles = perf_metrics.calculate_latency_percentiles()\n",
    "    \n",
    "    print(f\"\\nüìä LATENCY RESULTS:\")\n",
    "    print(f\"   Total queries tested: {len(latency_results)}\")\n",
    "    print(f\"   Mean latency: {percentiles['mean']:.2f} ms\")\n",
    "    print(f\"   Median (p50): {percentiles['p50']:.2f} ms\")\n",
    "    print(f\"   90th percentile (p90): {percentiles['p90']:.2f} ms\")\n",
    "    print(f\"   üéØ 95th percentile (p95): {percentiles['p95']:.2f} ms\")\n",
    "    print(f\"   üéØ 99th percentile (p99): {percentiles['p99']:.2f} ms\")\n",
    "    print(f\"   Min latency: {percentiles['min']:.2f} ms\")\n",
    "    print(f\"   Max latency: {percentiles['max']:.2f} ms\")\n",
    "    \n",
    "    # Category-wise analysis\n",
    "    print(f\"\\nüìã LATENCY BY QUERY CATEGORY:\")\n",
    "    for category, latencies in category_latencies.items():\n",
    "        if latencies:\n",
    "            avg_latency = statistics.mean(latencies)\n",
    "            p95_latency = np.percentile(latencies, 95)\n",
    "            print(f\"   {category:20} | Avg: {avg_latency:6.2f} ms | p95: {p95_latency:6.2f} ms | Count: {len(latencies)}\")\n",
    "    \n",
    "    return latency_results, percentiles, category_latencies\n",
    "\n",
    "# Run latency measurements\n",
    "if es and es.ping() and indexing_success:\n",
    "    latency_results, percentiles, category_latencies = measure_system_latency(\n",
    "        es, INDEX_NAME, diverse_queries, warmup_runs=5\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå Cannot measure latency - ES not available or indexing failed\")\n",
    "    latency_results = []\n",
    "    percentiles = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ MEASURING MEMORY FOOTPRINT (Metric C)\n",
      "==================================================\n",
      "üìä MEMORY FOOTPRINT RESULTS:\n",
      "   üñ•Ô∏è  Process Memory:\n",
      "       Current: 2522.67 MB\n",
      "       System %: 16.09%\n",
      "       Peak: 2591.62 MB\n",
      "       Growth: -69.12 MB\n",
      "   üíø Index Storage:\n",
      "       Index size: 140.90 MB\n",
      "       Documents: 50,000\n",
      "       Avg doc size: 2.89 KB\n",
      "       Efficiency: 354.9 docs/MB\n",
      "   üåê System Memory:\n",
      "       Total: 15.31 GB\n",
      "       Available: 2.14 GB\n",
      "       Usage: 86.0%\n",
      "\n",
      "üéØ MEMORY EFFICIENCY ASSESSMENT:\n",
      "   üëç Good: Moderate index size (140.9 MB)\n",
      "   ‚úÖ Excellent: High storage efficiency (354.9 docs/MB)\n"
     ]
    }
   ],
   "source": [
    "def measure_memory_footprint(es_client, index_name):\n",
    "    \"\"\"\n",
    "    Measure C: Memory footprint of the system\n",
    "    Includes process memory, index size, and memory growth during operations\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üíæ MEASURING MEMORY FOOTPRINT (Metric C)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get current process memory\n",
    "    process = psutil.Process()\n",
    "    process_memory = process.memory_info()\n",
    "    \n",
    "    # Get system memory\n",
    "    system_memory = psutil.virtual_memory()\n",
    "    \n",
    "    # Get Elasticsearch index stats\n",
    "    index_stats = es_client.get_index_stats(index_name)\n",
    "    \n",
    "    memory_metrics = {\n",
    "        'process_memory_mb': process_memory.rss / 1024 / 1024,\n",
    "        'process_memory_percent': process.memory_percent(),\n",
    "        'system_total_gb': system_memory.total / 1024 / 1024 / 1024,\n",
    "        'system_available_gb': system_memory.available / 1024 / 1024 / 1024,\n",
    "        'system_usage_percent': system_memory.percent\n",
    "    }\n",
    "    \n",
    "    if index_stats and 'indices' in index_stats:\n",
    "        index_info = index_stats['indices'][index_name]['total']\n",
    "        memory_metrics.update({\n",
    "            'index_size_mb': index_info['store']['size_in_bytes'] / 1024 / 1024,\n",
    "            'index_document_count': index_info['docs']['count'],\n",
    "            'index_deleted_docs': index_info['docs']['deleted'],\n",
    "            'avg_doc_size_kb': (index_info['store']['size_in_bytes'] / index_info['docs']['count']) / 1024 if index_info['docs']['count'] > 0 else 0\n",
    "        })\n",
    "    \n",
    "    # Calculate memory efficiency\n",
    "    if 'index_size_mb' in memory_metrics and 'index_document_count' in memory_metrics:\n",
    "        docs_per_mb = memory_metrics['index_document_count'] / memory_metrics['index_size_mb']\n",
    "        memory_metrics['documents_per_mb'] = docs_per_mb\n",
    "    \n",
    "    # Get memory usage history from performance metrics\n",
    "    memory_history = perf_metrics.get_memory_footprint()\n",
    "    if memory_history:\n",
    "        memory_metrics.update(memory_history)\n",
    "    \n",
    "    print(f\"üìä MEMORY FOOTPRINT RESULTS:\")\n",
    "    print(f\"   üñ•Ô∏è  Process Memory:\")\n",
    "    print(f\"       Current: {memory_metrics['process_memory_mb']:.2f} MB\")\n",
    "    print(f\"       System %: {memory_metrics['process_memory_percent']:.2f}%\")\n",
    "    \n",
    "    if memory_history:\n",
    "        print(f\"       Peak: {memory_history['peak_memory_mb']:.2f} MB\")\n",
    "        print(f\"       Growth: {memory_history['memory_growth_mb']:.2f} MB\")\n",
    "    \n",
    "    print(f\"   üíø Index Storage:\")\n",
    "    if 'index_size_mb' in memory_metrics:\n",
    "        print(f\"       Index size: {memory_metrics['index_size_mb']:.2f} MB\")\n",
    "        print(f\"       Documents: {memory_metrics['index_document_count']:,}\")\n",
    "        print(f\"       Avg doc size: {memory_metrics['avg_doc_size_kb']:.2f} KB\")\n",
    "        print(f\"       Efficiency: {memory_metrics['documents_per_mb']:.1f} docs/MB\")\n",
    "    \n",
    "    print(f\"   üåê System Memory:\")\n",
    "    print(f\"       Total: {memory_metrics['system_total_gb']:.2f} GB\")\n",
    "    print(f\"       Available: {memory_metrics['system_available_gb']:.2f} GB\")\n",
    "    print(f\"       Usage: {memory_metrics['system_usage_percent']:.1f}%\")\n",
    "    \n",
    "    # Memory efficiency assessment\n",
    "    if 'index_size_mb' in memory_metrics:\n",
    "        size_mb = memory_metrics['index_size_mb']\n",
    "        doc_count = memory_metrics['index_document_count']\n",
    "        \n",
    "        print(f\"\\nüéØ MEMORY EFFICIENCY ASSESSMENT:\")\n",
    "        if size_mb < 100:\n",
    "            print(f\"   ‚úÖ Excellent: Small index size ({size_mb:.1f} MB)\")\n",
    "        elif size_mb < 500:\n",
    "            print(f\"   üëç Good: Moderate index size ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Large: Significant index size ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        if memory_metrics['documents_per_mb'] > 50:\n",
    "            print(f\"   ‚úÖ Excellent: High storage efficiency ({memory_metrics['documents_per_mb']:.1f} docs/MB)\")\n",
    "        elif memory_metrics['documents_per_mb'] > 25:\n",
    "            print(f\"   üëç Good: Decent storage efficiency ({memory_metrics['documents_per_mb']:.1f} docs/MB)\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Low: Storage efficiency could be improved ({memory_metrics['documents_per_mb']:.1f} docs/MB)\")\n",
    "    \n",
    "    return memory_metrics\n",
    "\n",
    "# Measure memory footprint\n",
    "if es and es.ping() and indexing_success:\n",
    "    memory_metrics = measure_memory_footprint(es, INDEX_NAME)\n",
    "else:\n",
    "    print(\"‚ùå Cannot measure memory - ES not available or indexing failed\")\n",
    "    memory_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ MEASURING FUNCTIONAL METRICS (Metric D)\n",
      "==================================================\n",
      "üîç Testing search quality...\n",
      "\n",
      "üìù Query: 'anarchism'\n",
      "   üìä Total results: 25\n",
      "   üéØ Precision: 0.050\n",
      "   üìà Recall: 1.000\n",
      "   üîó F1-Score: 0.095\n",
      "   üèÜ Expected result 'Anarchism' found at rank 1\n",
      "   üìà Score range: 0.000\n",
      "   üìä Score std dev: 0.000\n",
      "   P@1: 1.000\n",
      "   P@3: 1.000\n",
      "   P@5: 1.000\n",
      "   P@10: 1.000\n",
      "\n",
      "üìù Query: 'political philosophy'\n",
      "   üìä Total results: 4801\n",
      "   üìä Coverage: ‚úÖ (4801 >= 10)\n",
      "   üìà Score range: 0.000\n",
      "   üìä Score std dev: 0.000\n",
      "   P@1: 1.000\n",
      "   P@3: 1.000\n",
      "   P@5: 1.000\n",
      "   P@10: 1.000\n",
      "\n",
      "üìù Query: 'artificial intelligence'\n",
      "   üìä Total results: 1200\n",
      "   üìä Coverage: ‚úÖ (1200 >= 5)\n",
      "   üìà Score range: 0.000\n",
      "   üìä Score std dev: 0.000\n",
      "   P@1: 1.000\n",
      "   P@3: 1.000\n",
      "   P@5: 1.000\n",
      "   P@10: 1.000\n",
      "\n",
      "üèÜ OVERALL FUNCTIONAL METRICS:\n",
      "   üìä Mean Average Precision: 0.050\n",
      "   üéØ Mean F1-Score: 0.095\n",
      "   üìà Coverage Rate: 1.000\n",
      "\n",
      "üéñÔ∏è  FUNCTIONAL QUALITY ASSESSMENT:\n",
      "   ‚ö†Ô∏è  Needs improvement: Low precision search results\n"
     ]
    }
   ],
   "source": [
    "def measure_functional_metrics(es_client, index_name, test_queries):\n",
    "    \"\"\"\n",
    "    Measure D: Functional metrics like precision, recall, and ranking measures\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"üéØ MEASURING FUNCTIONAL METRICS (Metric D)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define ground truth for evaluation (simplified)\n",
    "    # In a real system, you'd have human-annotated relevance judgments\n",
    "    ground_truth = {\n",
    "        'anarchism': {\n",
    "            'relevant_docs': ['12'],  # We know doc 12 is about Anarchism\n",
    "            'highly_relevant': ['12'],\n",
    "            'expected_top_result': 'Anarchism'\n",
    "        },\n",
    "        'political philosophy': {\n",
    "            'relevant_terms': ['anarchism', 'philosophy', 'political'],\n",
    "            'min_expected_results': 10\n",
    "        },\n",
    "        'artificial intelligence': {\n",
    "            'relevant_terms': ['artificial', 'intelligence', 'technology'],\n",
    "            'min_expected_results': 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    functional_results = {}\n",
    "    \n",
    "    print(\"üîç Testing search quality...\")\n",
    "    \n",
    "    for query_text, truth in ground_truth.items():\n",
    "        print(f\"\\nüìù Query: '{query_text}'\")\n",
    "        \n",
    "        try:\n",
    "            # Execute search\n",
    "            es_query = {\"query\": {\"match\": {\"text\": query_text}}, \"size\": 20}\n",
    "            results = es_client.search(index_name, es_query)\n",
    "            \n",
    "            if not results or 'hits' not in results:\n",
    "                print(f\"   ‚ùå No results returned\")\n",
    "                continue\n",
    "                \n",
    "            hits = results['hits']['hits']\n",
    "            total_results = results['hits']['total']['value']\n",
    "            \n",
    "            print(f\"   üìä Total results: {total_results}\")\n",
    "            \n",
    "            # Calculate metrics based on available ground truth\n",
    "            metrics = {}\n",
    "            \n",
    "            if 'relevant_docs' in truth:\n",
    "                # Precision and Recall calculation\n",
    "                retrieved_docs = [hit['_id'] for hit in hits]\n",
    "                relevant_docs = truth['relevant_docs']\n",
    "                \n",
    "                true_positives = len(set(retrieved_docs) & set(relevant_docs))\n",
    "                false_positives = len(set(retrieved_docs) - set(relevant_docs))\n",
    "                false_negatives = len(set(relevant_docs) - set(retrieved_docs))\n",
    "                \n",
    "                precision = true_positives / len(retrieved_docs) if retrieved_docs else 0\n",
    "                recall = true_positives / len(relevant_docs) if relevant_docs else 0\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                \n",
    "                metrics.update({\n",
    "                    'precision': precision,\n",
    "                    'recall': recall,\n",
    "                    'f1_score': f1_score,\n",
    "                    'true_positives': true_positives,\n",
    "                    'false_positives': false_positives,\n",
    "                    'false_negatives': false_negatives\n",
    "                })\n",
    "                \n",
    "                print(f\"   üéØ Precision: {precision:.3f}\")\n",
    "                print(f\"   üìà Recall: {recall:.3f}\")\n",
    "                print(f\"   üîó F1-Score: {f1_score:.3f}\")\n",
    "            \n",
    "            if 'expected_top_result' in truth:\n",
    "                # Ranking quality - check if expected result is in top positions\n",
    "                top_titles = []\n",
    "                for hit in hits[:5]:\n",
    "                    title = hit['_source'].get('title', '')\n",
    "                    top_titles.append(title)\n",
    "                \n",
    "                expected = truth['expected_top_result']\n",
    "                if any(expected.lower() in title.lower() for title in top_titles):\n",
    "                    rank_position = next(i for i, title in enumerate(top_titles) if expected.lower() in title.lower())\n",
    "                    metrics['expected_result_rank'] = rank_position + 1\n",
    "                    print(f\"   üèÜ Expected result '{expected}' found at rank {rank_position + 1}\")\n",
    "                else:\n",
    "                    metrics['expected_result_rank'] = None\n",
    "                    print(f\"   ‚ùå Expected result '{expected}' not in top 5\")\n",
    "            \n",
    "            if 'min_expected_results' in truth:\n",
    "                # Coverage - minimum expected results\n",
    "                min_expected = truth['min_expected_results']\n",
    "                metrics['coverage'] = total_results >= min_expected\n",
    "                print(f\"   üìä Coverage: {'‚úÖ' if metrics['coverage'] else '‚ùå'} ({total_results} >= {min_expected})\")\n",
    "            \n",
    "            # Score distribution analysis\n",
    "            if hits:\n",
    "                scores = [hit['_score'] for hit in hits[:10]]\n",
    "                metrics.update({\n",
    "                    'top_score': max(scores),\n",
    "                    'score_range': max(scores) - min(scores),\n",
    "                    'score_std': statistics.stdev(scores) if len(scores) > 1 else 0\n",
    "                })\n",
    "                \n",
    "                print(f\"   üìà Score range: {metrics['score_range']:.3f}\")\n",
    "                print(f\"   üìä Score std dev: {metrics['score_std']:.3f}\")\n",
    "            \n",
    "            # Relevance at different cut-offs (simplified)\n",
    "            cutoffs = [1, 3, 5, 10]\n",
    "            for k in cutoffs:\n",
    "                if len(hits) >= k:\n",
    "                    # Simple relevance: results with score > threshold are considered relevant\n",
    "                    score_threshold = 5.0  # Adjust based on your score ranges\n",
    "                    relevant_at_k = sum(1 for hit in hits[:k] if hit['_score'] > score_threshold)\n",
    "                    precision_at_k = relevant_at_k / k\n",
    "                    metrics[f'precision_at_{k}'] = precision_at_k\n",
    "                    print(f\"   P@{k}: {precision_at_k:.3f}\")\n",
    "            \n",
    "            functional_results[query_text] = metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error testing query '{query_text}': {e}\")\n",
    "            functional_results[query_text] = {'error': str(e)}\n",
    "    \n",
    "    # Calculate overall functional metrics\n",
    "    overall_metrics = {}\n",
    "    \n",
    "    # Average precision across all queries\n",
    "    precisions = [m.get('precision', 0) for m in functional_results.values() if 'precision' in m]\n",
    "    if precisions:\n",
    "        overall_metrics['mean_average_precision'] = statistics.mean(precisions)\n",
    "    \n",
    "    # Average F1 score\n",
    "    f1_scores = [m.get('f1_score', 0) for m in functional_results.values() if 'f1_score' in m]\n",
    "    if f1_scores:\n",
    "        overall_metrics['mean_f1_score'] = statistics.mean(f1_scores)\n",
    "    \n",
    "    # Coverage rate\n",
    "    coverage_results = [m.get('coverage', False) for m in functional_results.values() if 'coverage' in m]\n",
    "    if coverage_results:\n",
    "        overall_metrics['coverage_rate'] = sum(coverage_results) / len(coverage_results)\n",
    "    \n",
    "    print(f\"\\nüèÜ OVERALL FUNCTIONAL METRICS:\")\n",
    "    if 'mean_average_precision' in overall_metrics:\n",
    "        print(f\"   üìä Mean Average Precision: {overall_metrics['mean_average_precision']:.3f}\")\n",
    "    if 'mean_f1_score' in overall_metrics:\n",
    "        print(f\"   üéØ Mean F1-Score: {overall_metrics['mean_f1_score']:.3f}\")\n",
    "    if 'coverage_rate' in overall_metrics:\n",
    "        print(f\"   üìà Coverage Rate: {overall_metrics['coverage_rate']:.3f}\")\n",
    "    \n",
    "    # Functional quality assessment\n",
    "    print(f\"\\nüéñÔ∏è  FUNCTIONAL QUALITY ASSESSMENT:\")\n",
    "    if overall_metrics.get('mean_average_precision', 0) > 0.7:\n",
    "        print(f\"   ‚úÖ Excellent: High precision search results\")\n",
    "    elif overall_metrics.get('mean_average_precision', 0) > 0.5:\n",
    "        print(f\"   üëç Good: Decent precision search results\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Needs improvement: Low precision search results\")\n",
    "    \n",
    "    return functional_results, overall_metrics\n",
    "\n",
    "# Measure functional metrics\n",
    "if es and es.ping() and indexing_success:\n",
    "    functional_results, overall_functional_metrics = measure_functional_metrics(es, INDEX_NAME, diverse_queries)\n",
    "else:\n",
    "    print(\"‚ùå Cannot measure functional metrics - ES not available or indexing failed\")\n",
    "    functional_results = None\n",
    "    overall_functional_metrics = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìä COMPREHENSIVE PERFORMANCE REPORT - ESIndex-v1.0\n",
      "================================================================================\n",
      "\n",
      "üÖ∞Ô∏è  METRIC A: SYSTEM RESPONSE TIME (LATENCY)\n",
      "   üìä Query Performance:\n",
      "      ‚Ä¢ Mean Latency: 14.90 ms\n",
      "      ‚Ä¢ 95th Percentile (p95): 26.57 ms ‚≠ê\n",
      "      ‚Ä¢ 99th Percentile (p99): 27.42 ms ‚≠ê\n",
      "      ‚Ä¢ Total Queries: 43\n",
      "\n",
      "üÖ±Ô∏è  METRIC B: SYSTEM THROUGHPUT\n",
      "   üöÄ Query Performance:\n",
      "      ‚Ä¢ Single-threaded: 143.19 queries/second ‚≠ê\n",
      "      ‚Ä¢ Multi-threaded: 304.13 queries/second ‚≠ê\n",
      "      ‚Ä¢ Speedup Factor: 2.12x\n",
      "      ‚Ä¢ Thread Count: 4\n",
      "\n",
      "üÖ≤  METRIC C: MEMORY FOOTPRINT\n",
      "   üíæ Memory Usage:\n",
      "      ‚Ä¢ Process Memory: 2522.67 MB ‚≠ê\n",
      "      ‚Ä¢ Index Size: 140.90 MB ‚≠ê\n",
      "      ‚Ä¢ Storage Efficiency: 354.9 docs/MB\n",
      "      ‚Ä¢ Peak Memory: 2591.62 MB\n",
      "\n",
      "üÖ≥  METRIC D: FUNCTIONAL METRICS\n",
      "   üéØ Search Quality:\n",
      "      ‚Ä¢ Mean Average Precision: 0.050 ‚≠ê\n",
      "      ‚Ä¢ Mean F1-Score: 0.095 ‚≠ê\n",
      "      ‚Ä¢ Coverage Rate: 1.000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/performance_report_wikipedia_50000_20251028_174510.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m report\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Generate comprehensive report\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m final_report = \u001b[43mgenerate_comprehensive_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéØ ESIndex-v1.0 Performance Evaluation Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Ready for comparison with SelfIndex implementation\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mgenerate_comprehensive_report\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# Save comprehensive report\u001b[39;00m\n\u001b[32m     65\u001b[39m report_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/performance_report_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSELECTED_DATASET\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_DOCUMENTS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatetime.now().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m_\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreport_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     67\u001b[39m     json.dump(report, f, indent=\u001b[32m2\u001b[39m, default=\u001b[38;5;28mstr\u001b[39m)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müíæ Report saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/IIITH/Sem3/IRE/Assignments/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/performance_report_wikipedia_50000_20251028_174510.json'"
     ]
    }
   ],
   "source": [
    "def generate_comprehensive_report():\n",
    "    \"\"\"Generate comprehensive performance report with all metrics A, B, C, D\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üìä COMPREHENSIVE PERFORMANCE REPORT - ESIndex-v1.0\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    report = {\n",
    "        'system_name': 'ESIndex-v1.0',\n",
    "        'dataset': SELECTED_DATASET,\n",
    "        'document_count': MAX_DOCUMENTS,\n",
    "        'index_name': INDEX_NAME,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Metric A: Latency\n",
    "    if percentiles:\n",
    "        print(f\"\\nüÖ∞Ô∏è  METRIC A: SYSTEM RESPONSE TIME (LATENCY)\")\n",
    "        print(f\"   üìä Query Performance:\")\n",
    "        print(f\"      ‚Ä¢ Mean Latency: {percentiles['mean']:.2f} ms\")\n",
    "        print(f\"      ‚Ä¢ 95th Percentile (p95): {percentiles['p95']:.2f} ms ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ 99th Percentile (p99): {percentiles['p99']:.2f} ms ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ Total Queries: {percentiles['total_queries']}\")\n",
    "        report['metrics']['latency'] = percentiles\n",
    "    \n",
    "    # Metric B: Throughput\n",
    "    if throughput_metrics:\n",
    "        print(f\"\\nüÖ±Ô∏è  METRIC B: SYSTEM THROUGHPUT\")\n",
    "        print(f\"   üöÄ Query Performance:\")\n",
    "        print(f\"      ‚Ä¢ Single-threaded: {throughput_metrics['single_thread_qps']:.2f} queries/second ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ Multi-threaded: {throughput_metrics['multi_thread_qps']:.2f} queries/second ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ Speedup Factor: {throughput_metrics['speedup_factor']:.2f}x\")\n",
    "        print(f\"      ‚Ä¢ Thread Count: {throughput_metrics['thread_count']}\")\n",
    "        report['metrics']['throughput'] = throughput_metrics\n",
    "    \n",
    "    # Metric C: Memory\n",
    "    if memory_metrics:\n",
    "        print(f\"\\nüÖ≤  METRIC C: MEMORY FOOTPRINT\")\n",
    "        print(f\"   üíæ Memory Usage:\")\n",
    "        print(f\"      ‚Ä¢ Process Memory: {memory_metrics['process_memory_mb']:.2f} MB ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ Index Size: {memory_metrics.get('index_size_mb', 0):.2f} MB ‚≠ê\")\n",
    "        print(f\"      ‚Ä¢ Storage Efficiency: {memory_metrics.get('documents_per_mb', 0):.1f} docs/MB\")\n",
    "        if 'peak_memory_mb' in memory_metrics:\n",
    "            print(f\"      ‚Ä¢ Peak Memory: {memory_metrics['peak_memory_mb']:.2f} MB\")\n",
    "        report['metrics']['memory'] = memory_metrics\n",
    "    \n",
    "    # Metric D: Functional\n",
    "    if overall_functional_metrics:\n",
    "        print(f\"\\nüÖ≥  METRIC D: FUNCTIONAL METRICS\")\n",
    "        print(f\"   üéØ Search Quality:\")\n",
    "        if 'mean_average_precision' in overall_functional_metrics:\n",
    "            print(f\"      ‚Ä¢ Mean Average Precision: {overall_functional_metrics['mean_average_precision']:.3f} ‚≠ê\")\n",
    "        if 'mean_f1_score' in overall_functional_metrics:\n",
    "            print(f\"      ‚Ä¢ Mean F1-Score: {overall_functional_metrics['mean_f1_score']:.3f} ‚≠ê\")\n",
    "        if 'coverage_rate' in overall_functional_metrics:\n",
    "            print(f\"      ‚Ä¢ Coverage Rate: {overall_functional_metrics['coverage_rate']:.3f}\")\n",
    "        \n",
    "        # Functional assessment\n",
    "        precision = overall_functional_metrics.get('mean_average_precision', 0)\n",
    "        \n",
    "        report['metrics']['functional'] = overall_functional_metrics\n",
    "    \n",
    "    # Save comprehensive report\n",
    "    report_filename = f\"results/performance_report_{SELECTED_DATASET}_{MAX_DOCUMENTS}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_filename, 'w') as f:\n",
    "        json.dump(report, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Report saved to: {report_filename}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\n",
    "final_report = generate_comprehensive_report()\n",
    "\n",
    "print(f\"\\nüéØ ESIndex-v1.0 Performance Evaluation Complete!\")\n",
    "print(f\"   Ready for comparison with SelfIndex implementation\")\n",
    "print(f\"   All metrics (A,B,C,D) successfully measured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Search Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n",
    "# !docker run -d --name elasticsearch -p 9200:9200 -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n",
    "\n",
    "\n",
    "\n",
    "# ! docker run -d --name elasticsearch -p 9200:9200 -v esdata:/usr/share/elasticsearch/data -e \"discovery.type=single-node\" -e \"xpack.security.enabled=false\" docker.elastic.co/elasticsearch/elasticsearch:8.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
