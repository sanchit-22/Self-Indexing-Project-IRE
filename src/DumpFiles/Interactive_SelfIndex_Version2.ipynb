{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéõÔ∏è Interactive SelfIndex Configuration & Testing\n",
    "## Complete Control Over Data, Indexing, and Queries\n",
    "\n",
    "This notebook allows you to:\n",
    "- Choose different datasets\n",
    "- Configure index variants (x, y, z combinations)\n",
    "- Create custom queries\n",
    "- Measure performance metrics (A, B, C, D)\n",
    "- Compare multiple configurations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Part 0: Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing dependencies...\n",
      "‚úÖ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'nltk==3.9.1',\n",
    "    'datasets==4.1.1',\n",
    "    'matplotlib==3.10.6',\n",
    "    'numpy==2.3.3',\n",
    "    'pandas==2.3.3',\n",
    "    'scipy==1.16.2',\n",
    "    'psutil==5.9.6',\n",
    "    'tqdm==4.67.1',\n",
    "    'ipywidgets==8.0.0',\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è  Could not install {package}\")\n",
    "\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "import statistics\n",
    "import threading\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import traceback\n",
    "\n",
    "# Data & Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# UI & Widgets\n",
    "from IPython.display import display, HTML, clear_output, Markdown\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# System\n",
    "import psutil\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"‚úÖ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéõÔ∏è Part 1: Configuration Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration presets loaded\n"
     ]
    }
   ],
   "source": [
    "# Dataset presets\n",
    "DATASET_PRESETS = {\n",
    "    'wikipedia_small': {'num_docs': 100, 'desc': 'Wikipedia - Small (100 docs)'},\n",
    "    'wikipedia_medium': {'num_docs': 1000, 'desc': 'Wikipedia - Medium (1K docs)'},\n",
    "    'wikipedia_large': {'num_docs': 5000, 'desc': 'Wikipedia - Large (5K docs)'},\n",
    "    'wikipedia_xlarge': {'num_docs': 50000, 'desc': 'Wikipedia - XLarge (50K docs)'},\n",
    "    'custom_political': {'num_docs': 50, 'desc': 'Custom - Political Philosophy (50 docs)'},\n",
    "}\n",
    "\n",
    "# Index variants (x, y, z combinations)\n",
    "INDEX_VARIANTS = {\n",
    "    'Boolean_Basic': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - Basic'},\n",
    "    'Boolean_Compr_GapVByte': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'CODE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - GapVByte compression'},\n",
    "    'Boolean_Compr_NoCompr': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - NoCompr compression'},\n",
    "    'Boolean_Compr_Zlib': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'CLIB', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - Zlib compression'},\n",
    "    'Boolean_Custom_Zlib_DocAtTime_EarlyStop': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'CLIB', 'qproc': 'DOCatat', 'optim': 'EarlyStopping', 'desc': 'Boolean - Custom - Zlib - DocAtTime - EarlyStop'},\n",
    "    'Boolean_DS_Custom': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - Custom datastore'},\n",
    "    'Boolean_DS_Redis': {'info': 'BOOLEAN', 'dstore': 'DB2', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - Redis datastore'},\n",
    "    'Boolean_DS_SQLite': {'info': 'BOOLEAN', 'dstore': 'DB1', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - SQLite datastore'},\n",
    "    'Boolean_Opt_EarlyStop': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'EarlyStopping', 'desc': 'Boolean - EarlyStop'},\n",
    "    'Boolean_Opt_NoOpt': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - NoOpt'},\n",
    "    'Boolean_Opt_Skip': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Skipping', 'desc': 'Boolean - Skip'},\n",
    "    'Boolean_Opt_Thresh': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Thresholding', 'desc': 'Boolean - Thresh'},\n",
    "    'Boolean_QProc_DocAtTime': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'DOCatat', 'optim': 'Null', 'desc': 'Boolean - DocAtTime'},\n",
    "    'Boolean_QProc_TermAtTime': {'info': 'BOOLEAN', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'Boolean - TermAtTime'},\n",
    "    'TFIDF_Basic': {'info': 'TFIDF', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'TF-IDF - Basic'},\n",
    "    'TFIDF_Custom_GapVByte_TermAtTime': {'info': 'TFIDF', 'dstore': 'CUSTOM', 'compr': 'CODE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'TF-IDF - Custom - GapVByte - TermAtTime'},\n",
    "    'TFIDF_Redis_GapVByte_TermAtTime_Thresh': {'info': 'TFIDF', 'dstore': 'DB2', 'compr': 'CODE', 'qproc': 'TERMatat', 'optim': 'Thresholding', 'desc': 'TF-IDF - Redis - GapVByte - TermAtTime - Threshold'},\n",
    "    'TFIDF_SQLite_Zlib_DocAtTime': {'info': 'TFIDF', 'dstore': 'DB1', 'compr': 'CLIB', 'qproc': 'DOCatat', 'optim': 'Null', 'desc': 'TF-IDF - SQLite - Zlib - DocAtTime'},\n",
    "    'WordCount_Basic': {'info': 'WORDCOUNT', 'dstore': 'CUSTOM', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'WordCount - Basic'},\n",
    "    'WordCount_SQLite_NoCompr': {'info': 'WORDCOUNT', 'dstore': 'DB1', 'compr': 'NONE', 'qproc': 'TERMatat', 'optim': 'Null', 'desc': 'WordCount - SQLite - No compression'},\n",
    "}\n",
    "\n",
    "# Query presets\n",
    "QUERY_PRESETS = {\n",
    "    'political': {\n",
    "        'queries': ['anarchism', 'political philosophy', 'socialism', 'democracy', 'government', 'authority', 'freedom', 'society'],\n",
    "        'desc': 'Political philosophy queries'\n",
    "    },\n",
    "    'technology': {\n",
    "        'queries': ['artificial intelligence', 'machine learning', 'neural networks', 'deep learning', 'computer', 'data', 'algorithm', 'system'],\n",
    "        'desc': 'Technology queries'\n",
    "    },\n",
    "    'diverse': {\n",
    "        'queries': ['philosophy', 'science', 'history', 'culture', 'society', 'technology', 'politics', 'economics'],\n",
    "        'desc': 'Diverse general queries'\n",
    "    },\n",
    "    'simple': {\n",
    "        'queries': ['the', 'and', 'of', 'to', 'in', 'is', 'it', 'you', 'that', 'he'],\n",
    "        'desc': 'Simple common words'\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration presets loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad272eaaffc44738313a2bfa9be1d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>1Ô∏è‚É£  Select Configuration</h3>'), Dropdown(description='Dataset:', options=(('W‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881bc83688eb4f88a9ef21164091093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration panel ready\n"
     ]
    }
   ],
   "source": [
    "# Create interactive widgets\n",
    "dataset_dropdown = widgets.Dropdown(\n",
    "    options=[(v['desc'], k) for k, v in DATASET_PRESETS.items()],\n",
    "    value='wikipedia_small',\n",
    "    description='Dataset:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "index_dropdown = widgets.Dropdown(\n",
    "    options=[(v['desc'], k) for k, v in INDEX_VARIANTS.items()],\n",
    "    value='Boolean_Basic',  # Changed from 'Boolean_Custom_NoCompr' to a valid key\n",
    "    description='Index Type:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "query_dropdown = widgets.Dropdown(\n",
    "    options=[(v['desc'], k) for k, v in QUERY_PRESETS.items()],\n",
    "    value='political',\n",
    "    description='Queries:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "custom_queries_text = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter custom queries (one per line)',\n",
    "    description='Custom Queries:',\n",
    "    rows=4,\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "# Buttons\n",
    "create_index_button = widgets.Button(\n",
    "    description='üî® Create Index',\n",
    "    button_style='info',\n",
    ")\n",
    "\n",
    "measure_metrics_button = widgets.Button(\n",
    "    description='üìä Measure Metrics',\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "    description='üóëÔ∏è  Clear',\n",
    "    button_style='warning',\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Display configuration panel\n",
    "config_box = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>1Ô∏è‚É£  Select Configuration</h3>\"),\n",
    "    dataset_dropdown,\n",
    "    index_dropdown,\n",
    "    widgets.HTML(\"<h3>2Ô∏è‚É£  Select Queries</h3>\"),\n",
    "    query_dropdown,\n",
    "    widgets.HTML(\"<h4>Or enter custom queries (one per line):</h4>\"),\n",
    "    custom_queries_text,\n",
    "    widgets.HTML(\"<h3>3Ô∏è‚É£  Execute</h3>\"),\n",
    "    widgets.HBox([create_index_button, measure_metrics_button, clear_button]),\n",
    "])\n",
    "\n",
    "display(config_box)\n",
    "display(output_area)\n",
    "\n",
    "print(\"‚úÖ Configuration panel ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Session manager initialized\n"
     ]
    }
   ],
   "source": [
    "# Session state management\n",
    "class IndexingSession:\n",
    "    def __init__(self):\n",
    "        self.current_index = None\n",
    "        self.current_config = None\n",
    "        self.current_data = None\n",
    "        self.metrics = {}\n",
    "        self.query_results = {}\n",
    "    \n",
    "    def clear(self):\n",
    "        self.current_index = None\n",
    "        self.current_config = None\n",
    "        self.current_data = None\n",
    "        self.metrics = {}\n",
    "        self.query_results = {}\n",
    "\n",
    "session = IndexingSession()\n",
    "print(\"‚úÖ Session manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Part 2: Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_preprocessed_dataset(max_docs=None):\n",
    "    \"\"\"Load the preprocessed dataset from ElasticSearch.ipynb\"\"\"\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv('dataset/preprocessed_dataset.csv')\n",
    "        if max_docs:\n",
    "            df = df.head(max_docs)\n",
    "        \n",
    "        # Use original_text for indexing (not processed_tokens)\n",
    "        docs = [(str(row['id']), str(row['original_text'])) for _, row in df.iterrows()]\n",
    "        print(f\"‚úÖ Loaded {len(docs)} preprocessed documents from dataset/preprocessed_dataset.csv\")\n",
    "        return docs\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ö†Ô∏è  Preprocessed dataset not found. Using fallback data.\")\n",
    "        return load_political_data()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error loading preprocessed dataset: {e}. Using fallback data.\")\n",
    "        return load_political_data()\n",
    "\n",
    "def load_political_data():\n",
    "    \"\"\"Load custom political philosophy data\"\"\"\n",
    "    docs = [\n",
    "        ('1', 'anarchism is a political philosophy and movement skeptical of all authority'),\n",
    "        ('2', 'marxism represents socialist economic theories focused on worker liberation'),\n",
    "        ('3', 'capitalism is an economic system based on private property and markets'),\n",
    "        ('4', 'democracy is a form of government in which power rests with the people'),\n",
    "        ('5', 'socialism advocates for collective ownership of production means'),\n",
    "        ('6', 'fascism is an authoritarian far-right form of government'),\n",
    "        ('7', 'libertarianism emphasizes individual liberty and limited government'),\n",
    "        ('8', 'liberalism emphasizes individual rights and freedoms'),\n",
    "        ('9', 'conservatism emphasizes tradition and gradual change'),\n",
    "        ('10', 'communism seeks a classless society with common ownership'),\n",
    "    ]\n",
    "    print(f\"‚úÖ Loaded {len(docs)} custom political documents\")\n",
    "    return docs\n",
    "\n",
    "def load_wikipedia_data(num_docs=100):\n",
    "    \"\"\"Load Wikipedia data from preprocessed dataset\"\"\"\n",
    "    docs = load_preprocessed_dataset(num_docs)\n",
    "    if len(docs) < num_docs:\n",
    "        print(f\"‚ö†Ô∏è  Only {len(docs)} documents available, requested {num_docs}\")\n",
    "    return docs\n",
    "\n",
    "def load_data(dataset_key):\n",
    "    \"\"\"Load dataset based on preset\"\"\"\n",
    "    preset = DATASET_PRESETS[dataset_key]\n",
    "    \n",
    "    if 'custom' in dataset_key:\n",
    "        return load_political_data()\n",
    "    else:\n",
    "        return load_wikipedia_data(preset['num_docs'])\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üî® Part 3: Indexing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexing functions defined\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text for indexing\"\"\"\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        stemmer = PorterStemmer()\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word.isalpha()]\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Preprocessing error: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_index_from_config(dataset_key, variant_key, output_area):\n",
    "    \"\"\"Create index with specified configuration\"\"\"\n",
    "    \n",
    "    with output_area:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üî® CREATING INDEX\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        try:\n",
    "            # Load data\n",
    "            print(f\"\\n1Ô∏è‚É£  Loading data...\")\n",
    "            documents = load_data(dataset_key)\n",
    "            \n",
    "            if not documents:\n",
    "                print(\"‚ùå No documents loaded\")\n",
    "                return None\n",
    "            \n",
    "            # Prepare documents\n",
    "            print(f\"2Ô∏è‚É£  Preparing {len(documents)} documents...\")\n",
    "            prepared_docs = [(doc_id, text) for doc_id, text in documents]\n",
    "            \n",
    "            # Create index\n",
    "            print(f\"3Ô∏è‚É£  Creating index with variant: {INDEX_VARIANTS[variant_key]['desc']}\")\n",
    "            \n",
    "            config = INDEX_VARIANTS[variant_key]\n",
    "            \n",
    "            # Import here to avoid circular imports\n",
    "            from self_index import SelfIndex, create_self_index\n",
    "            \n",
    "            # Index documents using the helper function\n",
    "            start_time = time.time()\n",
    "            index_name = f\"index_{dataset_key}_{variant_key}_{int(time.time())}\"\n",
    "            \n",
    "            index = create_self_index(\n",
    "                index_id=index_name,\n",
    "                files=prepared_docs,\n",
    "                info=config['info'],\n",
    "                dstore=config['dstore'],\n",
    "                qproc=config.get('qproc', 'TERMatat'),\n",
    "                compr=config['compr'],\n",
    "                optim=config.get('optim', 'Null')\n",
    "            )\n",
    "            \n",
    "            index_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"4Ô∏è‚É£  Index created in {index_time:.3f} seconds\")\n",
    "            print(f\"   Index ID: {index.identifier_short}\")\n",
    "            print(f\"   Vocabulary size: {len(index.vocabulary)} terms\")\n",
    "            print(f\"   Documents indexed: {index.num_docs}\")\n",
    "            \n",
    "            # Save session state\n",
    "            session.current_index = index\n",
    "            session.current_config = {\n",
    "                'dataset': dataset_key,\n",
    "                'variant': variant_key,\n",
    "                'num_docs': len(documents),\n",
    "                'vocab_size': len(index.vocabulary),\n",
    "                'index_time': index_time,\n",
    "                'index_name': index_name\n",
    "            }\n",
    "            session.current_data = prepared_docs\n",
    "            \n",
    "            print(f\"\\n‚úÖ Index created successfully!\")\n",
    "            return index\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error creating index: {e}\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "print(\"‚úÖ Indexing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Part 4: Query & Metrics Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query and metrics functions defined\n"
     ]
    }
   ],
   "source": [
    "def run_queries(index, queries, output_area):\n",
    "    \"\"\"Run queries and measure latency\"\"\"\n",
    "    \n",
    "    with output_area:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üîç RUNNING QUERIES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not index:\n",
    "            print(\"‚ùå No index available\")\n",
    "            return {}\n",
    "        \n",
    "        results = {}\n",
    "        latencies = []\n",
    "        \n",
    "        print(f\"\\nüìã Running {len(queries)} queries...\\n\")\n",
    "        \n",
    "        for i, query in enumerate(tqdm(queries, desc=\"Executing queries\"), 1):\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                result_json = index.query(query)\n",
    "                query_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                result = json.loads(result_json)\n",
    "                \n",
    "                results[query] = {\n",
    "                    'num_results': result['num_results'],\n",
    "                    'time_ms': query_time,\n",
    "                    'top_result': result['results'][0]['doc_id'] if result['results'] else None,\n",
    "                }\n",
    "                \n",
    "                latencies.append(query_time)\n",
    "                \n",
    "            except Exception as e:\n",
    "                results[query] = {'error': str(e)}\n",
    "        \n",
    "        # Calculate statistics\n",
    "        if latencies:\n",
    "            stats_dict = {\n",
    "                'mean_latency': statistics.mean(latencies),\n",
    "                'median_latency': statistics.median(latencies),\n",
    "                'p95_latency': np.percentile(latencies, 95),\n",
    "                'p99_latency': np.percentile(latencies, 99),\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüìä Query Statistics:\")\n",
    "            print(f\"   Mean: {stats_dict['mean_latency']:.2f} ms\")\n",
    "            print(f\"   P95: {stats_dict['p95_latency']:.2f} ms ‚≠ê\")\n",
    "            print(f\"   P99: {stats_dict['p99_latency']:.2f} ms ‚≠ê\")\n",
    "            \n",
    "            session.metrics['latency'] = stats_dict\n",
    "        \n",
    "        print(f\"\\n‚úÖ Queries executed successfully\")\n",
    "        session.query_results = results\n",
    "        return results\n",
    "\n",
    "def measure_metrics(index, queries, output_area):\n",
    "    \"\"\"Measure Metrics A, B, C, D\"\"\"\n",
    "    \n",
    "    with output_area:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä MEASURING PERFORMANCE METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        if not index:\n",
    "            print(\"‚ùå No index available\")\n",
    "            return\n",
    "        \n",
    "        # Metric A: Latency\n",
    "        print(f\"\\nüÖ∞Ô∏è  METRIC A: LATENCY (Response Time)\")\n",
    "        latencies = []\n",
    "        for query in queries[:10]:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                index.query(query)\n",
    "                latencies.append((time.time() - start_time) * 1000)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if latencies:\n",
    "            metric_a = {\n",
    "                'mean': statistics.mean(latencies),\n",
    "                'p95': np.percentile(latencies, 95),\n",
    "                'p99': np.percentile(latencies, 99)\n",
    "            }\n",
    "            print(f\"   Mean: {metric_a['mean']:.2f} ms\")\n",
    "            print(f\"   P95: {metric_a['p95']:.2f} ms\")\n",
    "            print(f\"   P99: {metric_a['p99']:.2f} ms\")\n",
    "            session.metrics['a_latency'] = metric_a\n",
    "        \n",
    "        # Metric B: Throughput\n",
    "        print(f\"\\nüÖ±Ô∏è  METRIC B: THROUGHPUT (Queries/Second)\")\n",
    "        start_time = time.time()\n",
    "        query_count = 0\n",
    "        \n",
    "        while (time.time() - start_time) < 5:\n",
    "            query = queries[query_count % len(queries)]\n",
    "            try:\n",
    "                index.query(query)\n",
    "                query_count += 1\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        metric_b = query_count / elapsed\n",
    "        print(f\"   Throughput: {metric_b:.2f} qps\")\n",
    "        session.metrics['b_throughput'] = metric_b\n",
    "        \n",
    "        # Metric C: Memory\n",
    "        print(f\"\\nüÖ≤  METRIC C: MEMORY FOOTPRINT\")\n",
    "        process = psutil.Process()\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        vocab_size = len(index.vocabulary)\n",
    "        print(f\"   Memory: {memory_mb:.2f} MB\")\n",
    "        print(f\"   Vocabulary: {vocab_size} terms\")\n",
    "        session.metrics['c_memory'] = {'memory_mb': memory_mb, 'vocab_size': vocab_size}\n",
    "        \n",
    "        # Metric D: Functional\n",
    "        print(f\"\\nüÖ≥  METRIC D: FUNCTIONAL METRICS\")\n",
    "        results_with_data = sum(1 for r in session.query_results.values() if r.get('num_results', 0) > 0)\n",
    "        coverage = results_with_data / len(session.query_results) if session.query_results else 0\n",
    "        print(f\"   Queries with results: {results_with_data}/{len(session.query_results)}\")\n",
    "        print(f\"   Coverage: {coverage:.1%}\")\n",
    "        session.metrics['d_functional'] = {'coverage': coverage}\n",
    "        \n",
    "        print(f\"\\n‚úÖ All metrics measured\")\n",
    "\n",
    "print(\"‚úÖ Query and metrics functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üé® Part 5: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Visualization functions defined\n"
     ]
    }
   ],
   "source": [
    "def visualize_results(output_area):\n",
    "    \"\"\"Create comprehensive visualization\"\"\"\n",
    "    \n",
    "    with output_area:\n",
    "        if not session.query_results:\n",
    "            print(\"‚ùå No query results to visualize\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìà VISUALIZATION\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        gs = GridSpec(2, 2, figure=fig, hspace=0.3, wspace=0.3)\n",
    "        \n",
    "        # Query results distribution\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        queries = list(session.query_results.keys())[:10]\n",
    "        results_count = [session.query_results[q].get('num_results', 0) for q in queries]\n",
    "        ax1.barh(range(len(queries)), results_count, color='steelblue')\n",
    "        ax1.set_yticks(range(len(queries)))\n",
    "        ax1.set_yticklabels([q[:20] for q in queries], fontsize=9)\n",
    "        ax1.set_xlabel('Number of Results')\n",
    "        ax1.set_title('üìä Query Results Distribution')\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Latency distribution\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        latencies = [session.query_results[q].get('time_ms', 0) for q in queries]\n",
    "        ax2.bar(range(len(queries)), latencies, color='coral')\n",
    "        ax2.set_xticks(range(len(queries)))\n",
    "        ax2.set_xticklabels([q[:10] for q in queries], rotation=45, fontsize=8)\n",
    "        ax2.set_ylabel('Time (ms)')\n",
    "        ax2.set_title('‚è±Ô∏è  Query Latency Distribution')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Metrics summary\n",
    "        ax3 = fig.add_subplot(gs[1, :])\n",
    "        ax3.axis('off')\n",
    "        \n",
    "        metrics_text = \"üìä PERFORMANCE METRICS SUMMARY\\n\\n\"\n",
    "        \n",
    "        if 'a_latency' in session.metrics:\n",
    "            m = session.metrics['a_latency']\n",
    "            metrics_text += f\"üÖ∞Ô∏è  Latency: Mean={m['mean']:.2f}ms, P95={m['p95']:.2f}ms, P99={m['p99']:.2f}ms\\n\\n\"\n",
    "        \n",
    "        if 'b_throughput' in session.metrics:\n",
    "            metrics_text += f\"üÖ±Ô∏è  Throughput: {session.metrics['b_throughput']:.2f} qps\\n\\n\"\n",
    "        \n",
    "        if 'c_memory' in session.metrics:\n",
    "            m = session.metrics['c_memory']\n",
    "            metrics_text += f\"üÖ≤  Memory: {m['memory_mb']:.2f}MB, Vocab: {m['vocab_size']} terms\\n\\n\"\n",
    "        \n",
    "        if 'd_functional' in session.metrics:\n",
    "            m = session.metrics['d_functional']\n",
    "            metrics_text += f\"üÖ≥  Coverage: {m['coverage']:.1%}\"\n",
    "        \n",
    "        ax3.text(0.05, 0.5, metrics_text, fontsize=11, family='monospace',\n",
    "                verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        \n",
    "        plt.suptitle(\"üéõÔ∏è Interactive SelfIndex Results Dashboard\", fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n‚úÖ Visualization complete\")\n",
    "\n",
    "print(\"‚úÖ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîò Part 6: Button Event Handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Event handlers attached\n"
     ]
    }
   ],
   "source": [
    "def on_create_index_clicked(b):\n",
    "    \"\"\"Handle create index button click\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        dataset_key = dataset_dropdown.value\n",
    "        variant_key = index_dropdown.value\n",
    "        \n",
    "        index = create_index_from_config(dataset_key, variant_key, output_area)\n",
    "        \n",
    "        if index:\n",
    "            # Get queries\n",
    "            if custom_queries_text.value.strip():\n",
    "                queries = [q.strip() for q in custom_queries_text.value.split('\\n') if q.strip()]\n",
    "            else:\n",
    "                query_key = query_dropdown.value\n",
    "                queries = QUERY_PRESETS[query_key]['queries']\n",
    "            \n",
    "            # Run queries\n",
    "            run_queries(index, queries, output_area)\n",
    "\n",
    "def on_measure_metrics_clicked(b):\n",
    "    \"\"\"Handle measure metrics button click\"\"\"\n",
    "    with output_area:\n",
    "        if not session.current_index:\n",
    "            print(\"‚ùå No index created yet. Please create index first.\")\n",
    "            return\n",
    "        \n",
    "        # Get queries\n",
    "        if custom_queries_text.value.strip():\n",
    "            queries = [q.strip() for q in custom_queries_text.value.split('\\n') if q.strip()]\n",
    "        else:\n",
    "            query_key = query_dropdown.value\n",
    "            queries = QUERY_PRESETS[query_key]['queries']\n",
    "        \n",
    "        # Measure metrics\n",
    "        measure_metrics(session.current_index, queries, output_area)\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_results(output_area)\n",
    "\n",
    "def on_clear_clicked(b):\n",
    "    \"\"\"Handle clear button click\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "    session.clear()\n",
    "    print(\"üóëÔ∏è  Session cleared\")\n",
    "\n",
    "# Attach event handlers\n",
    "create_index_button.on_click(on_create_index_clicked)\n",
    "measure_metrics_button.on_click(on_measure_metrics_clicked)\n",
    "clear_button.on_click(on_clear_clicked)\n",
    "\n",
    "print(\"‚úÖ Event handlers attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Usage Guide\n",
    "\n",
    "### Quick Start:\n",
    "\n",
    "1. **Select Configuration** (from dropdowns above)\n",
    "   - Choose dataset (Wikipedia small/medium/large or custom)\n",
    "   - Select index variant (Boolean, WordCount, or TF-IDF)\n",
    "   - Select query preset or enter custom queries\n",
    "\n",
    "2. **Create Index**\n",
    "   - Click \"üî® Create Index\" button\n",
    "   - Indexes the dataset with selected configuration\n",
    "   - Automatically runs queries\n",
    "\n",
    "3. **Measure Metrics**\n",
    "   - Click \"üìä Measure Metrics\" button\n",
    "   - Measures Metrics A (latency), B (throughput), C (memory), D (functional)\n",
    "   - Shows visualization dashboard\n",
    "\n",
    "4. **Compare Variants**\n",
    "   - Change index type or dataset\n",
    "   - Click \"üî® Create Index\" again\n",
    "   - Compare results across runs\n",
    "\n",
    "### Metrics Explained:\n",
    "\n",
    "- **üÖ∞Ô∏è Metric A**: Latency (mean, p95, p99 response times in ms)\n",
    "- **üÖ±Ô∏è Metric B**: Throughput (queries executed per second)\n",
    "- **üÖ≤  Metric C**: Memory (process memory and vocabulary size)\n",
    "- **üÖ≥ Metric D**: Functional (query coverage percentage)\n",
    "\n",
    "### Custom Queries:\n",
    "\n",
    "Enter your own queries one per line in the \"Custom Queries\" text box. Examples:\n",
    "- `anarchism`\n",
    "- `political philosophy`\n",
    "- `machine learning`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
