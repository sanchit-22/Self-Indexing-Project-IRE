# Complete Codebase Explanation - Indexing and Retrieval System

## Table of Contents
1. [Architecture Overview](#architecture-overview)
2. [File-by-File Breakdown](#file-by-file-breakdown)
3. [How the System Works](#how-the-system-works)
4. [Making Changes for Different Data](#making-changes-for-different-data)
5. [Modifying Query System](#modifying-query-system)

---

## Architecture Overview

Your project has **TWO parallel systems** for indexing and searching:

### System 1: **ESIndex-v1.0** (Elasticsearch-based)
- Uses existing Elasticsearch service
- Works with pre-built search engine
- Faster to implement but less educational
- Uses HTTP requests to communicate with ES

### System 2: **SelfIndex-v1.0** (Custom-built)
- Your own implementation from scratch
- Better for learning
- More control over behavior
- Built in pure Python

Both inherit from **IndexBase** - a common interface/contract that defines what methods must exist.

---

## File-by-File Breakdown

### 1. **index_base.py** - The Base Contract (Abstract Interface)

```python
# THIS FILE DEFINES THE INTERFACE - Like a blueprint that says:
# "Any index system MUST implement these methods"
```

**Key Classes:**

#### Enumerations (Configuration Options)

```python
class IndexInfo(Enum):
    BOOLEAN = 1        # Just track which docs have the term
    WORDCOUNT = 2      # Also track how many times term appears
    TFIDF = 3          # Advanced: use TF-IDF scoring

class DataStore(Enum):
    CUSTOM = 1         # Store on disk as files
    DB1 = 2            # Use database 1 (not implemented)
    DB2 = 3            # Use database 2 (not implemented)

class Compression(Enum):
    NONE = 1           # No compression
    CODE = 2           # Custom compression (gap encoding)
    CLIB = 3           # Library compression (zlib)

class QueryProc(Enum):
    TERMatat = 'T'     # Process query term-by-term
    DOCatat = 'D'      # Process query document-by-document

class Optimizations(Enum):
    Null = '0'         # No optimization
    Skipping = 'sp'    # Skip documents
    Thresholding = 'th' # Only consider top-k docs
    EarlyStopping = 'es' # Stop early if enough results
```

#### IndexBase Class

```python
class IndexBase(ABC):
    """
    Abstract Base Class - defines the contract that all index implementations MUST follow
    """
    
    def __init__(self, core, info, dstore, qproc, compr, optim):
        # Creates identifier strings for tracking which variant you're using
        # Example: "SelfIndex_i1d1c2qTo0" 
        #          â†‘       â†‘ â†‘ â†‘ â†‘â†‘ â†‘ â†‘
        #    system  info datastore compression qproc optimizations
    
    @abstractmethod
    def create_index(index_id, files) -> None:
        """
        INPUT:  index_id = "my_index", 
                files = [("doc1", "content1"), ("doc2", "content2")]
        OUTPUT: Creates and stores the index
        """
        pass
    
    @abstractmethod
    def load_index(serialized_index_dump) -> None:
        """Loads previously created index from disk"""
        pass
    
    @abstractmethod
    def query(query_string) -> str:
        """INPUT: "anarchism", OUTPUT: JSON string with results"""
        pass
    
    @abstractmethod
    def update_index(index_id, remove_files, add_files) -> None:
        """Modify index: remove some docs, add new ones"""
        pass
    
    @abstractmethod
    def delete_index(index_id) -> None:
        """Remove index from disk"""
        pass
    
    @abstractmethod
    def list_indices() -> Iterable[str]:
        """Return list of all available indices"""
        pass
    
    @abstractmethod
    def list_indexed_files(index_id) -> Iterable[str]:
        """Return list of documents in this index"""
        pass
```

**Why this file matters:**
- Defines the "contract" that ES and SelfIndex must follow
- Ensures both systems have the same interface
- Makes them interchangeable in your code

---

### 2. **Ass1.ipynb** - The Main Jupyter Notebook (ESIndex Implementation)

This is the **primary notebook you're running**. Let me break it down by sections:

#### Section 1: Dataset Loading & Preprocessing

```python
# ðŸ”§ CONFIGURATION SECTION
SELECTED_DATASET = 'wikipedia'  # Change to 'news' for news data
MAX_DOCUMENTS = 50000           # How many documents to index
INDEX_NAME = "esindex-v1.0"    # Name for ES index

# STEP 1: Load data from HuggingFace
ds = load_dataset("wikimedia/wikipedia", "20231101.en", cache_dir=local_path)

# STEP 2: Preprocess text
def preprocess(text):
    text = text.lower()
    text = text.translate(punct_table)  # Remove punctuation
    tokens = word_tokenize(text)
    tokens = [stemmer.stem(word) for word in tokens 
              if word not in stop_words and word.isalpha()]
    return tokens

# STEP 3: Process all documents
processed_docs = []
for item in dataset:
    original_text = item['text']
    processed_tokens = preprocess(original_text)
    processed_docs.append({
        'id': item['id'],
        'original_text': original_text,
        'processed_tokens': processed_tokens,
        'title': item.get('title', '')
    })
```

**What happens here:**
1. Loads 50,000 Wikipedia documents
2. Preprocesses each one (tokenization, stemming, stopword removal)
3. Creates word frequency plots (before and after preprocessing)
4. Generates two plots saved as PNG files

**To use different data:**
```python
# Option A: Use News dataset (already configured)
SELECTED_DATASET = 'news'

# Option B: Load from custom source
def load_custom_data():
    documents = []
    # Load your CSV, JSON, or database
    for item in your_data_source:
        documents.append({
            'id': item['id'],
            'text': item['content'],
            'title': item.get('title', '')
        })
    return documents
```

---

#### Section 2: Elasticsearch Connection & Indexing

```python
class WorkingElasticsearch:
    """
    This creates HTTP requests to Elasticsearch directly.
    Elasticsearch runs in Docker on localhost:9200
    """
    
    def __init__(self, host="http://localhost:9200"):
        self.host = host
    
    def ping(self):
        """Check if ES is running"""
        response = requests.get(f"{self.host}/")
        return response.status_code == 200
    
    def create_index(self, index_name, body):
        """
        Create an index with specific settings:
        - number_of_shards: 1 (all data in one partition)
        - number_of_replicas: 0 (no backup copies)
        - refresh_interval: 30s (don't refresh too often for speed)
        """
        response = requests.put(f"{self.host}/{quote(index_name)}", 
                              json=body, timeout=30)
        return response.status_code in [200, 201]
    
    def bulk_index(self, docs):
        """
        Send many documents to ES at once.
        Uses NDJSON format (newline-delimited JSON):
        
        {"index": {"_index": "my_index", "_id": "doc1"}}
        {"text": "content...", "title": "title..."}
        {"index": {"_index": "my_index", "_id": "doc2"}}
        {"text": "content...", "title": "title..."}
        """
        # Construct bulk body
        bulk_lines = []
        for doc in docs:
            bulk_lines.append(json.dumps({"index": {...}}))
            bulk_lines.append(json.dumps({doc_content}))
        
        response = requests.post(f"{self.host}/_bulk", 
                               data=bulk_body, 
                               headers={'Content-Type': 'application/x-ndjson'})
        return response.status_code == 200
    
    def search(self, index_name, query_body):
        """
        Execute search query. Example:
        
        query_body = {
            "query": {
                "match": {"text": "anarchism"}
            },
            "size": 10
        }
        """
        response = requests.post(f"{self.host}/{quote(index_name)}/_search", 
                               json=query_body, timeout=30)
        return response.json()
```

**What happens here:**
1. Connects to Elasticsearch running in Docker
2. Creates index with optimized settings
3. Sends documents in batches (very important for large datasets!)
4. Returns search results

**To change indexing parameters:**
```python
# In optimized_bulk_indexing function:
mapping = {
    "mappings": {
        "properties": {
            "id": {"type": "keyword"},
            "text": {"type": "text", "analyzer": "standard"},
            # Add more fields here for your data
        }
    },
    "settings": {
        "number_of_shards": 1,           # Change for distributed indexing
        "number_of_replicas": 0,         # Change for backups
        "refresh_interval": "30s",       # Change refresh frequency
        "max_result_window": 10000,      # Max results per query
    }
}
```

---

#### Section 3: Metrics Measurement (A, B, C, D)

The notebook measures four key metrics:

##### **Metric A: Latency (p95, p99)**

```python
def measure_system_latency(es_client, index_name, query_set):
    """
    Measures response time for different query types
    """
    
    latency_results = []
    
    for query_info in query_set:
        start_time = time.time()
        
        # Execute query
        result = es_client.search(index_name, {
            "query": {"match": {"text": query_info['query']}},
            "size": 10
        })
        
        end_time = time.time()
        latency_ms = (end_time - start_time) * 1000
        
        latency_results.append(latency_ms)
    
    # Calculate percentiles
    sorted_times = sorted(latency_results)
    p95 = np.percentile(sorted_times, 95)  # 95% of queries faster than this
    p99 = np.percentile(sorted_times, 99)  # 99% of queries faster than this
    
    return {'p95': p95, 'p99': p99, 'mean': statistics.mean(sorted_times)}
```

**Interpretation:**
- **p95 = 19ms**: 95% of your queries return in 19ms or less
- **p99 = 26ms**: 99% of your queries return in 26ms or less
- Good for: SLAs like "95% of queries under 50ms"

##### **Metric B: Throughput (queries/second)**

```python
def measure_system_throughput(es_client, index_name, query_set, duration=30):
    """
    Measures how many queries can be executed per second
    """
    
    query_count = 0
    start_time = time.time()
    
    while (time.time() - start_time) < duration:
        query = query_set[query_count % len(query_set)]
        
        # Execute query
        result = es_client.search(index_name, {
            "query": {"match": {"text": query['query']}},
            "size": 5
        })
        
        query_count += 1
    
    # Calculate QPS
    elapsed = time.time() - start_time
    qps = query_count / elapsed
    
    return qps  # e.g., 91.63 queries per second
```

**Interpretation:**
- **Single-threaded: 91.63 qps**: Each query takes ~11ms
- **Multi-threaded: 375.91 qps**: 4 threads working in parallel
- **Speedup: 4.10x**: Near-linear scaling (good!)

##### **Metric C: Memory Footprint**

```python
def measure_memory_footprint(es_client, index_name):
    """
    Measures memory usage
    """
    
    # Process memory
    process = psutil.Process()
    memory_mb = process.memory_info().rss / 1024 / 1024
    
    # Index size on disk
    index_stats = es_client.get_index_stats(index_name)
    index_size_mb = index_stats['indices'][index_name]['total']['store']['size_in_bytes'] / 1024 / 1024
    
    # Storage efficiency
    doc_count = index_stats['indices'][index_name]['total']['docs']['count']
    docs_per_mb = doc_count / index_size_mb
    
    return {
        'memory_mb': memory_mb,
        'index_size_mb': index_size_mb,
        'docs_per_mb': docs_per_mb
    }
```

**Interpretation:**
- **Index size: 140.9 MB** for 50,000 documents
- **Efficiency: 354.9 docs/MB**: Very efficient storage
- Good for: Understanding scalability

##### **Metric D: Functional Metrics (Precision, Recall, F1)**

```python
def measure_functional_metrics(es_client, index_name):
    """
    Measures search quality
    """
    
    # Define what's relevant
    ground_truth = {
        'anarchism': {
            'relevant_docs': ['12'],  # We know doc 12 is about anarchism
            'expected_top_result': 'Anarchism'
        }
    }
    
    for query_text, truth in ground_truth.items():
        # Execute query
        results = es_client.search(index_name, {
            "query": {"match": {"text": query_text}},
            "size": 20
        })
        
        # Calculate metrics
        retrieved_docs = [hit['_id'] for hit in results['hits']['hits']]
        relevant_docs = truth['relevant_docs']
        
        true_positives = len(set(retrieved_docs) & set(relevant_docs))
        false_positives = len(set(retrieved_docs) - set(relevant_docs))
        
        precision = true_positives / len(retrieved_docs)
        recall = true_positives / len(relevant_docs)
        f1 = 2 * (precision * recall) / (precision + recall)
        
        return {'precision': precision, 'recall': recall, 'f1': f1}
```

**Interpretation:**
- **Precision = 0.05**: Of top 20 results, only 1 is relevant (low!)
- **Recall = 1.0**: Found all relevant documents (good!)
- **F1 = 0.095**: Harmonic mean shows overall poor quality

---

### 3. **index_base.py** Details

This file contains enum definitions that track your index configuration:

```python
# Example: Creating an index with specific configuration
idx = IndexBase(
    core='ESIndex',           # Using Elasticsearch
    info='BOOLEAN',           # Just track yes/no presence
    dstore='DB1',             # Store in database 1
    qproc='TERMatat',         # Process query term-by-term
    compr='NONE',             # No compression
    optim='Null'              # No optimization
)

# Results in identifier:
# ESIndex_i1d2c1qTo0
# â†‘       â†‘â†‘ â†‘â†‘ â†‘â†‘ â†‘â†‘
# system  information datastore query_processing optimization
```

---

## How the System Works (End-to-End)

### Complete Flow for ESIndex:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. DATA LOADING                                             â”‚
â”‚    Load 50,000 Wikipedia documents from HuggingFace        â”‚
â”‚    Create list of (doc_id, text) tuples                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PREPROCESSING                                            â”‚
â”‚    For each document:                                       â”‚
â”‚    - Lowercase text                                         â”‚
â”‚    - Remove punctuation                                     â”‚
â”‚    - Tokenize into words                                    â”‚
â”‚    - Remove stopwords (the, a, and, etc.)                  â”‚
â”‚    - Stem words (running â†’ run, trees â†’ tree)             â”‚
â”‚                                                             â”‚
â”‚    Result: List of (doc_id, [tokens], title)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ELASTICSEARCH INDEXING                                  â”‚
â”‚    For each preprocessed document:                         â”‚
â”‚    - Send to Elasticsearch via HTTP POST                   â”‚
â”‚    - Elasticsearch creates inverted index internally       â”‚
â”‚    - Documents stored with searchable fields               â”‚
â”‚                                                             â”‚
â”‚    Result: "esindex-v1.0" index in Elasticsearch          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. QUERYING                                                 â”‚
â”‚    User submits query: "anarchism"                         â”‚
â”‚    - Send to Elasticsearch: {"query": {"match": {...}}}   â”‚
â”‚    - Elasticsearch returns ranked documents                â”‚
â”‚    - Results include scores (relevance)                    â”‚
â”‚                                                             â”‚
â”‚    Result: [                                               â”‚
â”‚      {"title": "Anarchism", "score": 12.8, "_id": "12"},  â”‚
â”‚      {"title": "Ayn Rand", "score": 12.8, "_id": "456"}, â”‚
â”‚      ...                                                    â”‚
â”‚    ]                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. PERFORMANCE MEASUREMENT                                  â”‚
â”‚    - Metric A: Measure latency (p95, p99)                 â”‚
â”‚    - Metric B: Measure throughput (qps)                   â”‚
â”‚    - Metric C: Measure memory (MB)                        â”‚
â”‚    - Metric D: Measure search quality (precision/recall)  â”‚
â”‚                                                             â”‚
â”‚    Result: performance_report.json                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Making Changes for Different Data

### Change 1: Use Different Dataset

```python
# Current (Wikipedia)
SELECTED_DATASET = 'wikipedia'
ds = load_dataset("wikimedia/wikipedia", "20231101.en", cache_dir=local_path)

# Option 1: Use News Dataset
SELECTED_DATASET = 'news'
ds = load_dataset("webz/news", split='train')

# Option 2: Load from CSV
import pandas as pd
SELECTED_DATASET = 'custom_csv'
df = pd.read_csv('your_data.csv')
documents = []
for _, row in df.iterrows():
    documents.append({
        'id': row['id'],
        'text': row['content_column'],
        'title': row.get('title', '')
    })

# Option 3: Load from JSON
import json
with open('your_data.json') as f:
    data = json.load(f)
documents = [
    {'id': item['id'], 'text': item['text'], 'title': item.get('title', '')}
    for item in data
]
```

### Change 2: Modify Preprocessing

```python
# Current preprocessing
def preprocess(text):
    text = text.lower()
    text = text.translate(punct_table)
    tokens = word_tokenize(text)
    tokens = [stemmer.stem(word) for word in tokens 
              if word not in stop_words and word.isalpha()]
    return tokens

# Option 1: Keep stopwords
def preprocess_keep_stopwords(text):
    text = text.lower()
    text = text.translate(punct_table)
    tokens = word_tokenize(text)
    return tokens  # No stopword removal

# Option 2: More aggressive preprocessing
def preprocess_aggressive(text):
    text = text.lower()
    text = text.translate(punct_table)
    tokens = word_tokenize(text)
    tokens = [stemmer.stem(word) for word in tokens 
              if word not in stop_words and word.isalpha() and len(word) > 3]
    return tokens  # Remove short words too

# Option 3: Keep original text, don't stem
def preprocess_no_stem(text):
    text = text.lower()
    text = text.translate(punct_table)
    tokens = word_tokenize(text)
    tokens = [word for word in tokens 
              if word not in stop_words and word.isalpha()]
    return tokens
```

### Change 3: Add Custom Fields

```python
# Current: Only stores id, text, title, token_count

# Modified: Add more fields
def prepare_documents_extended(dataset, config, max_docs):
    processed_docs = []
    
    for item in dataset:
        text = item[config['text_field']]
        
        # Add date field (if available)
        date = item.get('date', '2025-01-01')
        
        # Add category (if available)
        category = item.get('category', 'uncategorized')
        
        # Add word count
        word_count = len(text.split())
        
        # Add character count
        char_count = len(text)
        
        processed_docs.append({
            'id': item[config['id_field']],
            'original_text': text[:10000],  # Truncate for ES
            'processed_tokens': preprocess(text),
            'title': item.get('title', ''),
            'date': date,
            'category': category,
            'word_count': word_count,
            'char_count': char_count
        })
    
    return processed_docs
```

### Change 4: Modify Elasticsearch Mapping

```python
# Current mapping
mapping = {
    "mappings": {
        "properties": {
            "id": {"type": "keyword"},
            "text": {"type": "text", "analyzer": "standard"},
            "title": {"type": "text"},
            "token_count": {"type": "integer"}
        }
    }
}

# Extended mapping with more field types
mapping_extended = {
    "mappings": {
        "properties": {
            "id": {"type": "keyword"},
            "text": {"type": "text", "analyzer": "standard"},
            "title": {"type": "text"},
            "token_count": {"type": "integer"},
            "date": {"type": "date"},           # NEW: date field
            "category": {"type": "keyword"},    # NEW: category field
            "word_count": {"type": "integer"},  # NEW: word count
            "char_count": {"type": "integer"},  # NEW: char count
            "source_url": {"type": "keyword"},  # NEW: url (for deduplication)
            "author": {"type": "keyword"}       # NEW: author field
        }
    }
}

# Then update indexing code:
if es_client.index_exists(index_name):
    es_client.delete_index(index_name)

if not es_client.create_index(index_name, mapping_extended):
    print("Failed to create index")
```

---

## Modifying Query System

### Current Query System

The notebook uses simple Elasticsearch queries:

```python
# Single term search
query = {"query": {"match": {"text": "anarchism"}}, "size": 10}

# Multi-term search (OR logic by default)
query = {"query": {"match": {"text": "political philosophy"}}, "size": 10}

# Range search
query = {"query": {"range": {"token_count": {"gte": 100, "lte": 1000}}}, "size": 10}
```

### Option 1: Add Boolean Query Support

```python
def search_with_boolean(es_client, index_name, query_string):
    """
    Support Boolean queries like:
    - "anarchism AND philosophy"
    - "politics OR government"
    - "society NOT authority"
    """
    
    # Parse the query string
    if ' AND ' in query_string:
        terms = query_string.split(' AND ')
        es_query = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"text": term.strip()}} for term in terms
                    ]
                }
            },
            "size": 20
        }
    
    elif ' OR ' in query_string:
        terms = query_string.split(' OR ')
        es_query = {
            "query": {
                "bool": {
                    "should": [
                        {"match": {"text": term.strip()}} for term in terms
                    ]
                }
            },
            "size": 20
        }
    
    elif ' NOT ' in query_string:
        parts = query_string.split(' NOT ')
        es_query = {
            "query": {
                "bool": {
                    "must": [{"match": {"text": parts[0].strip()}}],
                    "must_not": [{"match": {"text": parts[1].strip()}}]
                }
            },
            "size": 20
        }
    
    else:
        # Fallback to simple match
        es_query = {"query": {"match": {"text": query_string}}, "size": 20}
    
    return es_client.search(index_name, es_query)
```

### Option 2: Add Phrase Search

```python
def search_with_phrase(es_client, index_name, phrase):
    """
    Search for exact phrase:
    search_with_phrase(es, "esindex-v1.0", "political philosophy")
    """
    
    es_query = {
        "query": {
            "match_phrase": {  # Exact phrase matching
                "text": phrase
            }
        },
        "size": 20
    }
    
    return es_client.search(index_name, es_query)
```

### Option 3: Add Fuzzy Search (Typo Tolerance)

```python
def search_fuzzy(es_client, index_name, term, fuzziness=1):
    """
    Fuzzy search with typo tolerance:
    - fuzziness=1: Allow 1 character difference
    - fuzziness=2: Allow 2 character differences
    
    Example: "anarcsim" â†’ finds "anarchism"
    """
    
    es_query = {
        "query": {
            "match": {
                "text": {
                    "query": term,
                    "fuzziness": fuzziness
                }
            }
        },
        "size": 20
    }
    
    return es_client.search(index_name, es_query)
```

### Option 4: Add Field-Specific Search

```python
def search_by_field(es_client, index_name, field, value):
    """
    Search in specific field:
    - search_by_field(es, "esindex-v1.0", "title", "anarchism")
    - search_by_field(es, "esindex-v1.0", "category", "politics")
    """
    
    es_query = {
        "query": {
            "match": {
                field: value
            }
        },
        "size": 20
    }
    
    return es_client.search(index_name, es_query)
```

### Option 5: Add Advanced Search with Filters

```python
def search_advanced(es_client, index_name, text_query, filters=None):
    """
    Advanced search with filters:
    
    filters = {
        "date_range": {"gte": "2023-01-01", "lte": "2024-12-31"},
        "category": "politics",
        "min_word_count": 100
    }
    """
    
    es_query = {
        "query": {
            "bool": {
                "must": [
                    {"match": {"text": text_query}}
                ],
                "filter": []
            }
        },
        "size": 20
    }
    
    # Add filters
    if filters:
        if "date_range" in filters:
            es_query["query"]["bool"]["filter"].append({
                "range": {"date": filters["date_range"]}
            })
        
        if "category" in filters:
            es_query["query"]["bool"]["filter"].append({
                "term": {"category": filters["category"]}
            })
        
        if "min_word_count" in filters:
            es_query["query"]["bool"]["filter"].append({
                "range": {"word_count": {"gte": filters["min_word_count"]}}
            })
    
    return es_client.search(index_name, es_query)
```

### Option 6: Modify Query Set for Performance Testing

```python
# Current: Fixed set of 43 diverse queries

# Option: Generate dynamic query set based on your data
def generate_custom_query_set(es_client, index_name, num_queries=50):
    """
    Generate query set from actual index statistics
    """
    
    # Get top terms
    es_query = {
        "size": 0,
        "aggs": {
            "top_terms": {
                "terms": {
                    "field": "text",
                    "size": num_queries
                }
            }
        }
    }
    
    results = es_client.search(index_name, es_query)
    top_terms = [bucket['key'] for bucket in results['aggregations']['top_terms']['buckets']]
    
    queries = []
    
    # Single term queries
    for term in top_terms[:10]:
        queries.append({
            'query': term,
            'category': 'single_term',
            'purpose': f'Test retrieval of term: {term}'
        })
    
    # Multi-term queries
    for i in range(0, len(top_terms), 2):
        if i+1 < len(top_terms):
            query = f"{top_terms[i]} {top_terms[i+1]}"
            queries.append({
                'query': query,
                'category': 'multi_term',
                'purpose': f'Test multi-term retrieval'
            })
    
    return queries
```

---

## Summary of Key Changes

When you want to change the system for different data/queries:

| Aspect | File | How to Change |
|--------|------|---------------|
| **Data Source** | Ass1.ipynb Section 1 | Modify `SELECTED_DATASET`, loading function |
| **Preprocessing** | Ass1.ipynb Section 1 | Modify `preprocess()` function |
| **Fields Indexed** | Ass1.ipynb Section 2 | Add/remove properties in mapping |
| **Query Type** | Ass1.ipynb Section 3+ | Create new `search_*()` functions |
| **Performance Tests** | Ass1.ipynb Metrics | Modify query set generation |
| **Index Config** | Ass1.ipynb Section 2 | Modify settings in `create_index()` |
